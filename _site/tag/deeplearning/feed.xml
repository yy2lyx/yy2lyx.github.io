<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="http://localhost:4000/tag/deeplearning/feed.xml" rel="self" type="application/atom+xml" />
  <link href="http://localhost:4000/" rel="alternate" type="text/html" />
  <updated>2021-04-12T15:45:16+08:00</updated>
  <id>http://localhost:4000/tag/deeplearning/feed.xml</id>

  
  
  

  
    <title type="html">李小肥的YY | </title>
  

  
    <subtitle>欢迎各位看官光临本小站，希望共同学习进步哈！</subtitle>
  

  

  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  
  

  
    <entry>
      <title type="html">目标检测(one stage)-SSD</title>
      <link href="http://localhost:4000/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B_2" rel="alternate" type="text/html" title="目标检测(one stage)-SSD" />
      <published>2021-04-04T04:21:00+08:00</published>
      <updated>2021-04-04T04:21:00+08:00</updated>
      <id>http://localhost:4000/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B_2</id>
      <content type="html" xml:base="http://localhost:4000/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B_2">&lt;h3 id=&quot;一-yolo和ssd的对比&quot;&gt;一. YOLO和SSD的对比&lt;/h3&gt;

&lt;p&gt;yolo和ssd两个模型结构如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/04/06/uC7PiazmWIbq2kJ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;两个模型之间最主要的差别：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在特征抽取层其实相差不大：YOLO用的是器自己的conv架构；SSD用的是VGG-16&lt;/li&gt;
  &lt;li&gt;主要差别在结果预测上：YOLO用的是全连接层后得到7*7的grid，利用每个grid的boundingbox来做目标检测；SSD利用不同大小的feature map来做目标检测。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;二-模型结构&quot;&gt;二. 模型结构&lt;/h3&gt;

&lt;h4 id=&quot;21-特征抽取层&quot;&gt;2.1 特征抽取层&lt;/h4&gt;

&lt;p&gt;那么如何从VGG-16的结构变成SSD的结构呢?下图是一个VGG-16的示意图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/04/06/fKEV8UWLTvPGmQ3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;将VGG-16的最后一层pooling层变成3*3 的卷积层，再接一个atrous conv（空洞卷积）拿到不同大小的feature map。如下所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/04/06/qdFiaXwTlJUmS9p.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;22-空洞卷积&quot;&gt;2.2 空洞卷积&lt;/h4&gt;

&lt;p&gt;这里运用atrous conv layer而不是普通的conv layer的目的：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;在相同的感受野的同时，能获得更快的运算速度&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;如下图所示，是5 * 5 的卷积的kernel和3 * 3的atrous conv的kernel的感受野。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/04/06/HPzOvInK6fhstr2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到，如果是3 * 3的conv层接5 * 5的conv层，那么feature map中单一点的感受野其实是7个像素点；而如果是3 * 3的conv层接3 * 3的atrous conv层，能达到相同的感受野，且计算速度更快。&lt;/p&gt;

&lt;h4 id=&quot;22-推理层&quot;&gt;2.2 推理层&lt;/h4&gt;

&lt;p&gt;下图是SSD的推理层的示意图。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/04/06/83wrMCQ5YJajBGz.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到，图片经过vgg16之后，首先会得到较浅的feature map,随后经过几层卷积之后，得到较为深层的feature map（所以在上图中仅有较深层的能检测到车这种大物体），同时每层的feature map都会经过一个检测器和分类器得到检测结果，最后经过NMS得到最终的检测结果。&lt;/p&gt;

&lt;p&gt;那么整个SSD的anchor box的数量是：
&lt;script type=&quot;math/tex&quot;&gt;38*38*3+19*19*6+10*10*6+5*5*6+3*3*6+1*1*6 = 7308&lt;/script&gt;&lt;/p&gt;

&lt;h3 id=&quot;三-模型训练&quot;&gt;三. 模型训练&lt;/h3&gt;

&lt;h4 id=&quot;31训练loss&quot;&gt;3.1训练loss&lt;/h4&gt;

&lt;p&gt;SSD和YOLO的loss中的检测类别值有所不同：假定检测目标一共A个类别，那么YOLO的预测类别数位A个，而SSD的预测类别则是A+1个（包含了背景类）。如下图所示。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/04/06/nqH2h5B6CtupZSs.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;32-难负例挖掘&quot;&gt;3.2 难负例挖掘&lt;/h4&gt;

&lt;p&gt;对于正负样本不均衡的情况，SSD采用了hard negative mining(难负例挖掘)技巧来解决。hard negative是指在图片中容易将负样本（背景）看成是正样本（前景）的样本。而mining的操作就是将这类样本放入模型进行学习，从而减少模型的false positive。&lt;/p&gt;

&lt;p&gt;那么SSD是如何引用hard negative mining技巧呢？如下图，其中蓝色的box的我们希望它的confidence较低，而绿色的confidence较高。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;对于一张图而言，选出其中anchor box中negative置信度较高的box。&lt;/li&gt;
  &lt;li&gt;正负比例的anchor box = 1：3&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/04/06/Z8uVQvXyMs2SKhR.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;32-数据增强&quot;&gt;3.2 数据增强&lt;/h4&gt;

&lt;p&gt;SSD模型在论文中也使用了很多不同的data augmentation(数据增强)的操作。&lt;/p&gt;

&lt;p&gt;方式一：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;针对原始输入图片和ground truth进行IOU的操作&lt;/li&gt;
  &lt;li&gt;对其中iou = 0.1，0.3，0.5，0.7和0.9来进行采样。&lt;/li&gt;
  &lt;li&gt;对采样后的图片进行resize成相同大小的图片，然后进行水平翻转的操作。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/04/06/h4K3V1XzYRLFQwg.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;方式二（Random Expansion-得到的小目标训练样本）：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;对原始图像做不同比例的缩小。&lt;/li&gt;
  &lt;li&gt;然后放在相同大小图片中不同的地方。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/04/06/E48HkqysBPFw9JV.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;四-结果比较&quot;&gt;四. 结果比较&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/04/06/uKA6pBnV71YRFah.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;可以看到，SSD相较于YOLO在准确性上有很大的提升，同时预测速度上也能达到很高的fps。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name></name>
        
        
      </author>

      

      
        <category term="ComputerVision" />
      
        <category term="DeepLearning" />
      
        <category term="SSD" />
      

      
        <summary type="html">目标检测（one stage）——SSD</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">目标检测(one stage)-YOLOv1</title>
      <link href="http://localhost:4000/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B_1" rel="alternate" type="text/html" title="目标检测(one stage)-YOLOv1" />
      <published>2021-03-11T04:21:00+08:00</published>
      <updated>2021-03-11T04:21:00+08:00</updated>
      <id>http://localhost:4000/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B_1</id>
      <content type="html" xml:base="http://localhost:4000/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B_1">&lt;h3 id=&quot;一-目标检测算法的分类及历史&quot;&gt;一. 目标检测算法的分类及历史&lt;/h3&gt;

&lt;h4 id=&quot;11-目标检测算法的分类&quot;&gt;1.1 目标检测算法的分类&lt;/h4&gt;

&lt;p&gt;目标检测算法主要分为2大类：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;one-stage(one-shot object detectors) ：直接预测目标的bounding box及其类别。特点是一步到位，速度很快。比如：YOLO，SSD等系列模型。&lt;/li&gt;
  &lt;li&gt;two-stage：需要先使用启发式方法(selective search)或者CNN网络(RPN)产生Region Proposal，然后再在Region Proposal上做分类与回归。特点是：慢，但是准确率高。比如：RCNN系列模型。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;由于在工业应用中，往往对模型预测速度有要求，而two-stage目标检测模型由于先天的不足，因此本文仅考虑one-stage目标检测模型。&lt;/p&gt;

&lt;h4 id=&quot;12-目标检测发展流程&quot;&gt;1.2 目标检测发展流程&lt;/h4&gt;

&lt;p&gt;目标检测（one-stage）的总体发展流程：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;2015.06 — YOLOv1：第一个one-stage目标检测器。&lt;/li&gt;
  &lt;li&gt;2015.12 — SSD：结合anchor box和多尺度特征的one-stage目标检测器。&lt;/li&gt;
  &lt;li&gt;2016.12 — YOLOv2：YOLO的第二版。&lt;/li&gt;
  &lt;li&gt;2016.12 — FPN：特征金字塔（结合不同尺寸的特征图像）&lt;/li&gt;
  &lt;li&gt;2017.01 — DSSD：SSD结合FPN。&lt;/li&gt;
  &lt;li&gt;2017.08 — RetinaNet：Focal Loss解决正负样本不均衡&lt;/li&gt;
  &lt;li&gt;2018.04 — YOLOv3：YOLO的第三版。&lt;/li&gt;
  &lt;li&gt;2018.07 — CBAM：Attention机制的目标检测。&lt;/li&gt;
  &lt;li&gt;2019.11 — EfficientDet：Google提出的目标检测器。&lt;/li&gt;
  &lt;li&gt;2020.04 — YOLOv4：YOLO的第四版。&lt;/li&gt;
  &lt;li&gt;2020.06 — YOLOv5：YOLO第五版。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;二-yolo&quot;&gt;二. YOLO&lt;/h3&gt;

&lt;p&gt;当我最初学习图像分类的时候，就一直疑惑：如果我利用卷积层抽取目标特征后直接把分类任务做成回归任务（包含目标的位置和类别信息）可以作为目标检测器么？答案来了——YOLO（You Look Only Once）。&lt;/p&gt;

&lt;h4 id=&quot;21-模型结构&quot;&gt;2.1 模型结构&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/03/16/bAs2nLNVF5uWijZ.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;YOLO模型的结构如上所示：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;输入为一个448*448的一个图片输入。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;一共是经过24层的卷积层抽取特征，使用relu作为每一层的激活函数。&lt;/li&gt;
  &lt;li&gt;最后通过全连接层，且output形式为[7,7,30]的输出。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;模型输出的理解：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;将448 * 448的图像分为7 * 7的grid（网格），每个grid都会进行判断：是否为前景，且会构建2个boundingbox来框出物体。因此，一共是有7 * 7 * 2个框。而每个grid都会输出x,y,w,h,c；这里的confidence的计算就是前景目标的概率 * iou的值。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/03/16/p5KV6fZGaQCqUMT.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;除了boundingbox的计算外，当然还需要输出目标是哪个类别，即输出检测到的目标是某个类别的概率。这样就可以计算每个grid属于某个类别下的iou情况了。&lt;/li&gt;
  &lt;li&gt;最后利用NMS（非极大值抑制：顾名思义就是不是最大的置信度就不要了）找到每个目标的最合适的框。具体NMS的算法步骤如下：
    &lt;ul&gt;
      &lt;li&gt;（1）首先拿到的是YOLO模型输出的结果，即7 * 7 * 2个框，每个框都是由5个元素（x,y,w,h,c）。这里需要知道一张图片中有多少个目标且目标confidence最高的结果。&lt;/li&gt;
      &lt;li&gt;（2）通过计算两两框之间的IOU（交并比），用来划分一张图片中有多少个目标（如果IOU&amp;gt;0说明属于同一目标下的框）。&lt;/li&gt;
      &lt;li&gt;（3）对同一目标下的所有框的confidence进行排序，找到最大的的confidence对应的框。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;22-模型训练&quot;&gt;2.2 模型训练&lt;/h4&gt;

&lt;p&gt;这里主要讲述模型训练过程中loss的定义过程。&lt;/p&gt;

&lt;h5 id=&quot;221-location-loss&quot;&gt;2.2.1 Location Loss&lt;/h5&gt;

&lt;p&gt;定义如下所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/03/16/ZLlQj29WeVTdzRI.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/03/16/oeTrzxY4auHEG8c.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如上所示，假定是将图片划分为3 * 3个grid，每个grid有且仅有一个预测框，由于只计算和前景目标匹配的框，因此只会计算grid5和grid7的location loss。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;grid5的loss：&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/03/16/G7TUCNdS5lWDKrw.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;grid7的loss：&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/03/16/nBHViDx8kpRjGZ9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;但是这里看大大猫和小猫的loss竟然是一样的，大猫的loss应该明显要小一些，而小猫的loss明显要大一些。因此这种loss的计算还需要提升。这里就将w,h的分别先进行&lt;strong&gt;开根号&lt;/strong&gt;处理。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;grid5的loss：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/03/16/ZA2t1zlJKIu9XMD.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;grid7的loss：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/03/16/hlRF8OMHrt6wXnk.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;222-object-loss&quot;&gt;2.2.2 Object Loss&lt;/h5&gt;

&lt;p&gt;定义如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/03/16/N9KPlvfTF1OCuWE.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;那么上图的每个grid的confidence的值如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/03/16/jlszbUxcCQTLqIK.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;object loss的值为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/03/16/HvNnV8h6eZdoWE7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;但是这个是只划分了3 * 3个grid的，那么如果是原论文中的7 * 7的情况下呢，此时的object loss的值为：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/03/16/WqYcKHxbveoRhQL.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;我们可以看到，0.96这个检测的背景的loss就过大了，那么在反向传播的过程中，梯度的变化很大程度就着重在背景的部分，以至于学习前景的能力较差。&lt;/p&gt;

&lt;p&gt;因此，重新定义object loss（其实就是在背景loss引入一个系数，比如0.5）：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/03/16/ctsYBoIguWzlab7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;223-classification-loss&quot;&gt;2.2.3 classification loss&lt;/h5&gt;

&lt;p&gt;定义如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/03/16/fI6jQpviDU5Kswo.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;24-yolo存在问题&quot;&gt;2.4 YOLO存在问题&lt;/h4&gt;

&lt;h5 id=&quot;241-同一个grid却是多个目标的中心点&quot;&gt;2.4.1 同一个grid却是多个目标的中心点&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/03/16/iLWdGr6kxPAUplD.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如上图所示，人和车的中心点基本都落在中心的grid中，对于yolo而言，就无法分辨到底是人还是车？一个grid下只能预测1个目标。&lt;/p&gt;

&lt;h5 id=&quot;242-同一个grid中存在多个小目标&quot;&gt;2.4.2 同一个grid中存在多个小目标&lt;/h5&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/03/16/UmrSYCLP7MdTXab.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如上图所示，同一个grid下有多个鸟（小目标），而对于yolo而言，一个grid下只能预测1个目标。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name></name>
        
        
      </author>

      

      
        <category term="yolo" />
      
        <category term="ComputerVision" />
      
        <category term="DeepLearning" />
      

      
        <summary type="html">目标检测（one stage）的开始——YOLOv1</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Libtorch的GPU使用问题记录</title>
      <link href="http://localhost:4000/Libtorch%E7%9A%84GPU%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95" rel="alternate" type="text/html" title="Libtorch的GPU使用问题记录" />
      <published>2021-01-31T04:21:00+08:00</published>
      <updated>2021-01-31T04:21:00+08:00</updated>
      <id>http://localhost:4000/Libtorch%E7%9A%84GPU%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95</id>
      <content type="html" xml:base="http://localhost:4000/Libtorch%E7%9A%84GPU%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95">&lt;blockquote&gt;
  &lt;p&gt;这里得吹逼下自己领导，10min解决了困扰我2天的问题（好吧，也许是我太蠢）。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;一-问题描述&quot;&gt;一. 问题描述&lt;/h3&gt;

&lt;p&gt;由于项目需要使用libtorch（pytorch的C++版本）的GPU版本，但是发现无法使用GPU，因此将问题和解决过程记录下来，方便日后观看和反思。&lt;/p&gt;

&lt;h3 id=&quot;二-解决问题的过程&quot;&gt;二. 解决问题的过程&lt;/h3&gt;

&lt;h4 id=&quot;21-使用的torch版本&quot;&gt;2.1 使用的torch版本&lt;/h4&gt;

&lt;p&gt;这里需要说下pytorch和libtorch的版本一定要一致，且和cuda的版本一致。这里都是通过pytorch官网上进行安装即可。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;pytorch1.6.0（GPU）：使用pip安装&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# CUDA 10.1
pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;pytorch1.6.0（CPU）：使用pip安装&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# CPU only
pip install torch==1.6.0+cpu torchvision==0.7.0+cpu -f https://download.pytorch.org/whl/torch_stable.html
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;libtorch1.6.0（GPU）：选择使用release版本即可（据说debug有问题）&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;https://download.pytorch.org/libtorch/cu101/libtorch-win-shared-with-deps-1.6.0%2Bcu101.zip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;libtorch1.6.0（CPU）：选择使用release版本即可（据说debug有问题）&lt;/p&gt;
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;https://download.pytorch.org/libtorch/cpu/libtorch-win-shared-with-deps-1.6.0%2Bcpu.zip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;22-使用cmakelist的搭建工程&quot;&gt;2.2 使用cmakelist的搭建工程&lt;/h4&gt;

&lt;div class=&quot;language-cmake highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cmake_minimum_required&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;VERSION 3.12 FATAL_ERROR&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;project&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;torch_gpu_test&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# add CMAKE_PREFIX_PATH&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;APPEND CMAKE_PREFIX_PATH &lt;span class=&quot;s2&quot;&gt;&quot;D:/software/opencv/opencv/build/x64/vc15/lib&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;APPEND CMAKE_PREFIX_PATH &lt;span class=&quot;s2&quot;&gt;&quot;D:/software/libtorch_gpu&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;APPEND CUDA_TOOLKIT_ROOT_DIR &lt;span class=&quot;s2&quot;&gt;&quot;D:/software/cuda/development&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;nb&quot;&gt;find_package&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Torch REQUIRED&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;find_package&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;OpenCV REQUIRED&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;NOT Torch_FOUND&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;FATAL_ERROR &lt;span class=&quot;s2&quot;&gt;&quot;Pytorch Not Found!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;endif&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;NOT Torch_FOUND&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;STATUS &lt;span class=&quot;s2&quot;&gt;&quot;Pytorch status:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;STATUS &lt;span class=&quot;s2&quot;&gt;&quot;    libraries: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;TORCH_LIBRARIES&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;STATUS &lt;span class=&quot;s2&quot;&gt;&quot;Find Torch VERSION: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;Torch_VERSION&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;STATUS &lt;span class=&quot;s2&quot;&gt;&quot;OpenCV library status:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;STATUS &lt;span class=&quot;s2&quot;&gt;&quot;    version: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;OpenCV_VERSION&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;STATUS &lt;span class=&quot;s2&quot;&gt;&quot;    libraries: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;OpenCV_LIBS&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;STATUS &lt;span class=&quot;s2&quot;&gt;&quot;    include path: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;OpenCV_INCLUDE_DIRS&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;nb&quot;&gt;add_executable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;torch_gpu_test torch_gpu_test.cpp&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;target_link_libraries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;torch_gpu_test &lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;TORCH_LIBRARIES&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;OpenCV_LIBS&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set_property&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;TARGET torch_gpu_test PROPERTY CXX_STANDARD 11&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;这里利用vs2019生成项目之后，编写以下代码进行测试：&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;#include &amp;lt;torch/torch.h&amp;gt;
#include &amp;lt;torch/script.h&amp;gt;

using namespace torch;

int main()
{
    torch::DeviceType device_type = at::kCPU;
    if (torch::cuda::is_available()) {
        cout &amp;lt;&amp;lt; &quot;cuda!&quot; &amp;lt;&amp;lt; endl;
        torch::DeviceType device_type = at::kCUDA;
    }
    else
    {
        cout &amp;lt;&amp;lt; &quot;cpu&quot; &amp;lt;&amp;lt; endl;
    }
    
}
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
  &lt;li&gt;到了这里我开始了我的问题之旅：由于是Release版本，不能debug，只能主观的认为这里应该是cuda的环境没配好导致torch无法使用gpu的，因此一直在找cmake的cuda环境配置问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;23-release-with-debug改变了我的想法&quot;&gt;2.3 Release with Debug改变了我的想法&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;这里得说下本人第一次知道release版本也可以debug！（本人也算一C++小白哈，别计较）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;这里顺带记录下如何使用vs2019的Release with debug的过程：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;直接在项目中将&lt;code class=&quot;highlighter-rouge&quot;&gt;Release&lt;/code&gt;版本选择为&lt;code class=&quot;highlighter-rouge&quot;&gt;RelWithDebInfo&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/02/01/FcP7UNy4fqTxkR1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;禁用代码优化功能：这里是防止出现“变量已被优化掉 因而不可用”这种问题&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://i.loli.net/2021/02/01/KSBZQE8f7IkenWD.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在这里debug的时候发现，&lt;code class=&quot;highlighter-rouge&quot;&gt;device&lt;/code&gt;这个我定义的变量是可以加载cuda的！因此可以推翻我之前想的（cuda环境的问题）。&lt;/p&gt;

&lt;h4 id=&quot;24-libtorch16gpu版本问题&quot;&gt;2.4 libtorch1.6GPU版本问题&lt;/h4&gt;

&lt;p&gt;这里就可以肯定是libtorch的GPU问题了。为啥&lt;code class=&quot;highlighter-rouge&quot;&gt;torch::cuda::is_available()&lt;/code&gt;会是&lt;code class=&quot;highlighter-rouge&quot;&gt;false&lt;/code&gt;呢？&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;网上的思路是：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在“属性 –&amp;gt; 链接器 –&amp;gt; 命令行 –&amp;gt; 其他选项”中添加：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;  /INCLUDE:?warp_size@cuda@at@@YAHXZ
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;本人实验了下，按照网上的添加会报错，因此以下是本人实验可行的结果：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;在“链接器 –&amp;gt; 输入 –&amp;gt; 附加依赖项”中进行添加：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;D:\software\libtorch_gpu\lib\torch_cuda.lib
D:\software\libtorch_gpu\lib\torch_cpu.lib
-INCLUDE:?warp_size@cuda@at@@YAHXZ
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;这里很奇怪，cmakelist明明已经配置好了libtorch的gpu，但是这里却没有&lt;code class=&quot;highlighter-rouge&quot;&gt;torch_cuda.lib&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;至此，问题解决了！&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name></name>
        
        
      </author>

      

      
        <category term="pytorch" />
      
        <category term="DeepLearning" />
      

      
        <summary type="html">介绍pytorch的C++版本的gpu使用的解决问题的过程记录</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">GCN在多标签分类中的应用</title>
      <link href="http://localhost:4000/GCN%E5%9C%A8%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8" rel="alternate" type="text/html" title="GCN在多标签分类中的应用" />
      <published>2021-01-11T04:21:00+08:00</published>
      <updated>2021-01-11T04:21:00+08:00</updated>
      <id>http://localhost:4000/GCN%E5%9C%A8%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8</id>
      <content type="html" xml:base="http://localhost:4000/GCN%E5%9C%A8%E5%A4%9A%E6%A0%87%E7%AD%BE%E5%88%86%E7%B1%BB%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8">&lt;h3 id=&quot;一--torch的图神经网络库pyg&quot;&gt;一.  Torch的图神经网络库pyG&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;torch_geometric 官方文档：https://pytorch-geometric.readthedocs.io/en/latest/index.html&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;11-安装及使用&quot;&gt;1.1 安装及使用&lt;/h4&gt;

&lt;p&gt;这里参考官网的安装过程。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;确定自己安装的pytorch版本：&lt;code class=&quot;highlighter-rouge&quot;&gt;pip list&lt;/code&gt;进行查看，例如本人的torch版本为&lt;code class=&quot;highlighter-rouge&quot;&gt;1.6.0+cu101&lt;/code&gt;（这里的&lt;code class=&quot;highlighter-rouge&quot;&gt;cu101&lt;/code&gt;是指cuda10.1）&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;安装相关的第三方包，这里注意要匹配上面的torch版本，因此：&lt;code class=&quot;highlighter-rouge&quot;&gt;${TORCH} = 1.6.0&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;${CUDA} = cu101&lt;/code&gt;&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install &lt;span class=&quot;nt&quot;&gt;--no-index&lt;/span&gt; torch-scatter &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://pytorch-geometric.com/whl/torch-&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;TORCH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;+&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CUDA&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;.html
pip install &lt;span class=&quot;nt&quot;&gt;--no-index&lt;/span&gt; torch-sparse &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://pytorch-geometric.com/whl/torch-&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;TORCH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;+&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CUDA&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;.html
pip install &lt;span class=&quot;nt&quot;&gt;--no-index&lt;/span&gt; torch-cluster &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://pytorch-geometric.com/whl/torch-&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;TORCH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;+&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CUDA&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;.html
pip install &lt;span class=&quot;nt&quot;&gt;--no-index&lt;/span&gt; torch-spline-conv &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; https://pytorch-geometric.com/whl/torch-&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;TORCH&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;+&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CUDA&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;.html
pip install torch-geometric
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;安装完成之后，测试下&lt;code class=&quot;highlighter-rouge&quot;&gt;import torch_geometric&lt;/code&gt;导包没有报错说明安装完成了。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;12-图数据导入&quot;&gt;1.2 图数据导入&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;&lt;em&gt;torch_geometric.data&lt;/em&gt;&lt;/strong&gt;这个模块包含了一个叫Data的类，而这个类可以很方便的构建属于自己的数据集。&lt;code class=&quot;highlighter-rouge&quot;&gt;data&lt;/code&gt;实例有以下属性：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt;：节点的特征矩阵，shape = [节点个数，节点的特征数]。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;edge_index&lt;/code&gt;：这里可以理解为图的邻接矩阵，但是要注意这里要将邻接矩阵转换成COO格式，shape = [2, 边的数量]，type = torch.long。&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;edge_attr&lt;/code&gt;：边的特征矩阵，shape = [边的个数，边的特征数]&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;y&lt;/code&gt;：标签，如果任务是图分类，shape = [1, 图的标签数]；如果是节点分类，shape = [节点个数，节点的标签数]。（这里注意一哈：在torch中如果是多分类任务，不用转成onehot形式哦，因此标签数为1）&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;is_directed()&lt;/code&gt;：是否是有向图&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(1) 下面是edge_index的具体从邻接矩阵生成COO模式的代码。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;scipy.sparse&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;coo_matrix&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# 转化成COO格式&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;coo_A&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;coo_matrix&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;adj_arr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;edge_index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;coo_A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;coo_A&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;(2) 构建自己的数据集，只需要用list来封装这些Data即可。具体代码如下：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edge_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;13-图数据的转换及展示&quot;&gt;1.3 图数据的转换及展示&lt;/h4&gt;

&lt;p&gt;我们可以利用&lt;a href=&quot;https://pypi.org/project/networkx/&quot;&gt;networkx&lt;/a&gt;来对Data这个图进行展示和转换成networkx的图结构。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch_geometric.utils.convert&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to_networkx&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;networkx&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;draw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;G&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;to_networkx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;draw&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;nx&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write_gexf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;G&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;test.gexf&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;path.png&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;同时，还可以将gexf格式的图数据文件经过&lt;a href=&quot;https://gephi.org/users/download/&quot;&gt;Gephi&lt;/a&gt;这个开源的图数据展示软件来进行节点的渲染。&lt;/p&gt;

&lt;h3 id=&quot;二-图卷积网络gcn在多标签分类中的应用&quot;&gt;二. 图卷积网络GCN在多标签分类中的应用&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;论文参考：&lt;a href=&quot;https://arxiv.org/abs/1609.02907&quot;&gt;Semi-Supervised Classification with Graph Convolutional Networks&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;21-gcn在模型应用上的优缺点&quot;&gt;2.1 GCN在模型应用上的优缺点。&lt;/h4&gt;

&lt;p&gt;本次探究的是图卷积网络在图分类（多标签）上的应用，因此不涉及到节点的分类任务。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GCN的优点&lt;/strong&gt;：可以捕捉图的全局信息，很好的表征节点的特征，边的特征。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GCN的缺点&lt;/strong&gt;：若是新增节点，整个图发生变化， 那么GCN的结构就会发生变化。因此对于节点不固定的图结构来说，不适用。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GCN的主要作用&lt;/strong&gt;：抽取图中节点的拓扑信息（节点的邻接信息）。这里学到的是每个节点的一个唯一确定的embedding。如下图所示，多层的GCN抽取的是每个节点的唯一确定的embedding。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://tva1.sinaimg.cn/large/008eGmZEgy1gmugxnmbmhj30aw055aag.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;GCN的特性&lt;/strong&gt;：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;局部参数共享，算子是适用于每个节点（圆圈代表算子），处处共享。&lt;/li&gt;
  &lt;li&gt;感受域正比于层数，最开始的时候，每个节点包含了直接邻居的信息，再计算第二层时就能把邻居的邻居的信息包含进来，这样参与运算的信息就更多更充分。层数越多，感受域就更广，参与运算的信息就更多。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;22-gcn在图分类的模型搭建&quot;&gt;2.2 GCN在图分类的模型搭建&lt;/h4&gt;

&lt;p&gt;图分类任务下的模型搭建过程如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://tva1.sinaimg.cn/large/008eGmZEgy1gmugybl0ydj30k00b7gm2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;因此，利用pytorch_geometric来搭建图分类任务（多标签）的模型。这里代码中引入了两次图卷积和池化。在输入的数据中，除了包含节点的特征，还包含了边的特征。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch.nn.functional&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch_geometric.nn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GraphConv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TopKPooling&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch_geometric.nn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;global_mean_pool&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gap&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch_geometric.nn&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;global_max_pool&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gmp&lt;/span&gt;


&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;Net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Module&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;multi_label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Net&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GraphConv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# self.conv1.weight.data.normal_()&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pool1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TopKPooling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ratio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;GraphConv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pool2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TopKPooling&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ratio&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lin1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;16&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lin2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;64&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lin3&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Linear&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;128&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;multi_label&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;forward&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edge_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edge_attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edge_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edge_attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edge_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edge_attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edge_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edge_attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pool1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edge_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edge_attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edge_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;edge_attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edge_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edge_attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pool2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edge_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;edge_attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gmp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;gap&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dim&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x2&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lin1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lin2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dropout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sigmoid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lin3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;23-多标签multi-label分类任务&quot;&gt;2.3 多标签（Multi-Label）分类任务&lt;/h4&gt;

&lt;p&gt;在多标签分类任务中：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;输入的y：shape = [batch_size, multi-label的个数]，其中multi-label的形式都是[0,1,0,0,1….]，即每个类别之间都互不影响，且结果只有0和1。这里在&lt;code class=&quot;highlighter-rouge&quot;&gt;torch_geometric.data.y&lt;/code&gt;的shape = [1,multi-label的个数]。&lt;/li&gt;
  &lt;li&gt;分类模型的最后一层激活函数：&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.sigmoid()&lt;/code&gt;函数（即二分类常用的激活函数）,这里对于多标签分类任务同样适用。&lt;/li&gt;
  &lt;li&gt;损失函数的定义：&lt;code class=&quot;highlighter-rouge&quot;&gt; torch.nn.BCELoss()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;准确率的定义：在训练的时候，一般除了看训练集和验证集的loss以外，acc其实也可以当作模型好坏的指标。但是对于多标签分类而言，这里和一般的多分类，二分类任务定义的准确率不太一样。个人的理解（可能不对蛤）：对于一个样本（多标签）而言，有且仅有每个标签都预测对了，这个样本才能算预测正确了，因此，定义了以下acc。&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;where&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;acc_thread&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ones_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros_like&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;acc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;int&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;equal&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;acc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;epoch_accuracy&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;acc&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content>

      
      
      
      
      

      <author>
          <name></name>
        
        
      </author>

      

      
        <category term="pytorch" />
      
        <category term="GCN" />
      
        <category term="图算法" />
      
        <category term="DeepLearning" />
      

      
        <summary type="html">介绍在图卷积网络在多标签分类任务中的应用</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">windows下搭建libtorch和paddle的C++环境搭建</title>
      <link href="http://localhost:4000/windows-%E4%B8%8B%E6%90%AD%E5%BB%BAlibtorch%E5%92%8Cpaddle%E7%9A%84C++%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA" rel="alternate" type="text/html" title="windows下搭建libtorch和paddle的C++环境搭建" />
      <published>2020-12-26T04:21:00+08:00</published>
      <updated>2020-12-26T04:21:00+08:00</updated>
      <id>http://localhost:4000/windows%20%E4%B8%8B%E6%90%AD%E5%BB%BAlibtorch%E5%92%8Cpaddle%E7%9A%84C++%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA</id>
      <content type="html" xml:base="http://localhost:4000/windows-%E4%B8%8B%E6%90%AD%E5%BB%BAlibtorch%E5%92%8Cpaddle%E7%9A%84C++%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA">&lt;blockquote&gt;
  &lt;p&gt;参考文章：&lt;a href=&quot;https://pytorch.org/cppdocs/installing.html&quot;&gt;NSTALLING C++ DISTRIBUTIONS OF PYTORCH&lt;/a&gt;，&lt;a href=&quot;https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-rc1/guides/05_inference_deployment/inference/windows_cpp_inference.html&quot;&gt;安装与编译 Windows 预测库&lt;/a&gt;，&lt;a href=&quot;https://pytorch.apachecn.org/docs/1.0/cpp_export.html&quot;&gt;在C++中加载PYTORCH模型&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;一-必要软件&quot;&gt;一. 必要软件&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://visualstudio.microsoft.com/zh-hans/vs/&quot;&gt;vs2019&lt;/a&gt;：paddle和torch这里的编译都是由Visual Studio 2019完成的&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://pytorch.org/get-started/locally/&quot;&gt;libtorch&lt;/a&gt;：直接在官网上进行下载压缩包，这里说明下分为release和debug版本，直接下载release版本即可。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.paddlepaddle.org.cn/documentation/docs/zh/2.0-rc1/guides/05_inference_deployment/inference/windows_cpp_inference.html&quot;&gt;paddle&lt;/a&gt;：这里选择2.0-rc1的cpu版本的直接进行解压安装。&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://opencv.org/releases/&quot;&gt;opencv&lt;/a&gt;：windows下直接安装exe到本地即可。&lt;/li&gt;
  &lt;li&gt;cmake：直接用scoop安装&lt;code class=&quot;highlighter-rouge&quot;&gt;scoop install cmake&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;二-安装libtorch环境&quot;&gt;二. 安装libtorch环境&lt;/h3&gt;

&lt;h4 id=&quot;21-构建一个c项目&quot;&gt;2.1 构建一个C++项目&lt;/h4&gt;

&lt;p&gt;目录层级如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;├─example-app
	 ├─build // 新建一个空目录
	 ├─CMakeLists.txt // 构建一个cmakelist
	 └─example-app.cpp // 构建一个cpp文件用于测试
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;其中，&lt;code class=&quot;highlighter-rouge&quot;&gt;CMakeList.txt&lt;/code&gt;具体设置如下：&lt;/p&gt;

&lt;div class=&quot;language-cmake highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nb&quot;&gt;cmake_minimum_required&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;VERSION 3.12 FATAL_ERROR&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;project&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;example-app&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# add CMAKE_PREFIX_PATH&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#增加opencv和libtorch的路径&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;APPEND CMAKE_PREFIX_PATH &lt;span class=&quot;s2&quot;&gt;&quot;D:/software/opencv/opencv/build/x64/vc15/lib&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;span class=&quot;c1&quot;&gt;# 注意这里如果是vs2015的版本，需要改成 /build/x64/vc14/lib&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;APPEND CMAKE_PREFIX_PATH &lt;span class=&quot;s2&quot;&gt;&quot;D:/software/libtorch&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;nb&quot;&gt;find_package&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;Torch REQUIRED&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;find_package&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;OpenCV REQUIRED&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;if&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;NOT Torch_FOUND&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;FATAL_ERROR &lt;span class=&quot;s2&quot;&gt;&quot;Pytorch Not Found!&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;endif&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;NOT Torch_FOUND&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;STATUS &lt;span class=&quot;s2&quot;&gt;&quot;Pytorch status:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;STATUS &lt;span class=&quot;s2&quot;&gt;&quot;    libraries: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;TORCH_LIBRARIES&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;STATUS &lt;span class=&quot;s2&quot;&gt;&quot;OpenCV library status:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;STATUS &lt;span class=&quot;s2&quot;&gt;&quot;    version: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;OpenCV_VERSION&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;STATUS &lt;span class=&quot;s2&quot;&gt;&quot;    libraries: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;OpenCV_LIBS&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;message&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;STATUS &lt;span class=&quot;s2&quot;&gt;&quot;    include path: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;OpenCV_INCLUDE_DIRS&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;nb&quot;&gt;add_executable&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;example-app example-app.cpp&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;target_link_libraries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;example-app &lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;TORCH_LIBRARIES&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;OpenCV_LIBS&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set_property&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;TARGET example-app PROPERTY CXX_STANDARD 11&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;C++测试代码（&lt;code class=&quot;highlighter-rouge&quot;&gt;example-app.cpp&lt;/code&gt;）如下（测试opencv和libtorch）：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;#include &amp;lt;torch/torch.h&amp;gt;
#include &amp;lt;iostream&amp;gt;
#include &amp;lt;opencv2/core.hpp&amp;gt;
#include &amp;lt;opencv2/highgui/highgui.hpp&amp;gt;

using namespace std;
using namespace cv;

int main() {
  torch::Tensor tensor = torch::rand({2, 3});
  std::cout &amp;lt;&amp;lt; tensor &amp;lt;&amp;lt; std::endl;
  std::cout &amp;lt;&amp;lt; &quot;ok!&quot; &amp;lt;&amp;lt; std::endl;
  Mat img = imread(&quot;1.jpg&quot;);
  imshow(&quot;1&quot;,img);
  waitKey(0);
  return 0;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h4 id=&quot;22-编译和生成项目&quot;&gt;2.2 编译和生成项目&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;进入到&lt;code class=&quot;highlighter-rouge&quot;&gt;build&lt;/code&gt;目录：&lt;code class=&quot;highlighter-rouge&quot;&gt;cd build&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;利用cmake进行编译： &lt;code class=&quot;highlighter-rouge&quot;&gt;cmake ..&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;编译顺利的话，就可以看到&lt;code class=&quot;highlighter-rouge&quot;&gt;build&lt;/code&gt;目录下生成了如下所示：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://tva1.sinaimg.cn/large/0081Kckwgy1gmaqn8ispmj30j706dq3s.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;利用vs2019打开项目&lt;code class=&quot;highlighter-rouge&quot;&gt;example-app.sln&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;点击&lt;code class=&quot;highlighter-rouge&quot;&gt;example-app&lt;/code&gt; 右键选择&lt;code class=&quot;highlighter-rouge&quot;&gt;设为启动项&lt;/code&gt;，并且将版本选择&lt;code class=&quot;highlighter-rouge&quot;&gt;release&lt;/code&gt;版本，点击&lt;code class=&quot;highlighter-rouge&quot;&gt;本地Windows调试器&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://tva1.sinaimg.cn/large/0081Kckwgy1gmaqnkotpsj30kk0blmxs.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;23-调试问题的解决&quot;&gt;2.3 调试问题的解决&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;报错信息：&lt;code class=&quot;highlighter-rouge&quot;&gt;由于找不到c10.dll&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.dll&lt;/code&gt;这种找不到dll文件的，直接将dll文件(这些dll文件都在&lt;code class=&quot;highlighter-rouge&quot;&gt;libtorch/lib&lt;/code&gt;路径下)复制到&lt;code class=&quot;highlighter-rouge&quot;&gt;build/release&lt;/code&gt;文件夹下&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;opencv_world3411.dll&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;opencv_ffmpeg3411_64.dll&lt;/code&gt;等都在opencv的&lt;code class=&quot;highlighter-rouge&quot;&gt;opencv\opencv\build\x64\vc15\lib&lt;/code&gt;路径下。&lt;/li&gt;
  &lt;li&gt;这里注意测试opencv的时候，需要将图片放置到和&lt;code class=&quot;highlighter-rouge&quot;&gt;example-app.vcxproj&lt;/code&gt;同级目录下&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;24-exe生成文件的平台移植&quot;&gt;2.4 exe生成文件的平台移植&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;如果需要将生成的exe文件移植到其他PC上面，只需要将release文件夹下所有文件（包括dll文件和exe文件）复制到其他PC即可。&lt;/li&gt;
  &lt;li&gt;生成的exe文件在找图片的时候也是同级目录下找，因此需要将图片放置到&lt;code class=&quot;highlighter-rouge&quot;&gt;exe&lt;/code&gt;文件的同级目录下。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;25-pytorch模型在c平台的使用&quot;&gt;2.5 pytorch模型在C++平台的使用&lt;/h4&gt;

&lt;p&gt;PyTorch模型从Python到C++的转换由&lt;a href=&quot;https://pytorch.org/docs/master/jit.html&quot;&gt;Torch Script&lt;/a&gt;实现。Torch Script是PyTorch模型的一种表示，可由Torch Script编译器理解，编译和序列化。一般利用trace将PyTorch模型转换为Torch脚本,必须将模型的实例以及样本输入传递给&lt;code class=&quot;highlighter-rouge&quot;&gt;torch.jit.trace&lt;/code&gt;函数。这将生成一个 &lt;code class=&quot;highlighter-rouge&quot;&gt;torch.jit.ScriptModule&lt;/code&gt;对象，并在模块的&lt;code class=&quot;highlighter-rouge&quot;&gt;forward&lt;/code&gt;方法中嵌入模型评估的跟踪。&lt;/p&gt;

&lt;h3 id=&quot;三-安装paddle的c环境&quot;&gt;三. 安装paddle的C++环境&lt;/h3&gt;

&lt;h4 id=&quot;31-下载安装paddle&quot;&gt;3.1 下载安装paddle&lt;/h4&gt;

&lt;p&gt;这里官网有2种方式在windows上安装paddle环境：一个是通过git下载paddle源码进行编译安装，另一种直接从官网下载zip编译好的文件（本文使用该种方式）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://tva1.sinaimg.cn/large/0081Kckwgy1gmaqnvgko9j30q707iaac.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;32-结合paddleocr测试并使用paddle预测库&quot;&gt;3.2 结合paddleOCR测试并使用paddle预测库&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;paddleOCR的git地址：https://github.com/PaddlePaddle/PaddleOCR&lt;/li&gt;
  &lt;li&gt;下载到本地之后，&lt;code class=&quot;highlighter-rouge&quot;&gt;cd PaddleOCR\deploy\cpp_infer&lt;/code&gt;，修改&lt;code class=&quot;highlighter-rouge&quot;&gt;CMakeList.txt&lt;/code&gt;文件&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-cmake highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nf&quot;&gt;SET&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;PADDLE_LIB &lt;span class=&quot;s2&quot;&gt;&quot;D:/software/paddle_inference_install_dir&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 这里是下载的paddle预测库的路径&lt;/span&gt;
&lt;span class=&quot;nf&quot;&gt;SET&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;OPENCV_DIR &lt;span class=&quot;s2&quot;&gt;&quot;D:/software/opencv/opencv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# 这里是下载的opencv的路径&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;find_package&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;OpenCV REQUIRED&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;新建一个build文件夹：&lt;code class=&quot;highlighter-rouge&quot;&gt;mkdir build&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;进入build：&lt;code class=&quot;highlighter-rouge&quot;&gt;cd build&lt;/code&gt; ， 编译：&lt;code class=&quot;highlighter-rouge&quot;&gt;cmake ..&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;同样的利用vs2019打开项目&lt;code class=&quot;highlighter-rouge&quot;&gt;ocr_system.sln&lt;/code&gt;，生成即可。&lt;/li&gt;
  &lt;li&gt;这里注意需要将&lt;code class=&quot;highlighter-rouge&quot;&gt;paddle_fluid.dll&lt;/code&gt;放入到&lt;code class=&quot;highlighter-rouge&quot;&gt;Release&lt;/code&gt;目录下。&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name></name>
        
        
      </author>

      

      
        <category term="pytorch" />
      
        <category term="paddle" />
      
        <category term="Cplusplus" />
      
        <category term="DeepLearning" />
      

      
        <summary type="html">介绍在C++平台下搭建torch和paddle的环境</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Keras训练模型部署到C++环境并生成DLL</title>
      <link href="http://localhost:4000/Keras%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%88%B0C++%E7%8E%AF%E5%A2%83%E7%94%9F%E6%88%90DLL" rel="alternate" type="text/html" title="Keras训练模型部署到C++环境并生成DLL" />
      <published>2020-11-03T22:21:00+08:00</published>
      <updated>2020-11-03T22:21:00+08:00</updated>
      <id>http://localhost:4000/Keras%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%88%B0C++%E7%8E%AF%E5%A2%83%E7%94%9F%E6%88%90DLL</id>
      <content type="html" xml:base="http://localhost:4000/Keras%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%E5%88%B0C++%E7%8E%AF%E5%A2%83%E7%94%9F%E6%88%90DLL">&lt;h3 id=&quot;一-准备工作&quot;&gt;一. 准备工作&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;在Windows上搭建完成Tensorflow的C++环境，这里参考本人的上一篇博客&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;在windows上部署opencv3的C++环境&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;python3的keras和tf2.X的环境：&lt;code class=&quot;highlighter-rouge&quot;&gt;pip3 install keras==2.4.3&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;pip3 install tensorflow==2.3.1&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;python3的opencv环境：&lt;code class=&quot;highlighter-rouge&quot;&gt;pip3 install opencv-python&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;二-python模型训练&quot;&gt;二. python模型训练&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;python模型构建：&lt;a href=&quot;https://www.kaggle.com/tongpython/cat-and-dog&quot;&gt;Kaggle猫狗分类&lt;/a&gt;&lt;/p&gt;

  &lt;p&gt;python完整代码放在本人的&lt;a href=&quot;&quot;&gt;git仓库&lt;/a&gt;上面&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;21-模型训练保存h5格式的模型文件&quot;&gt;2.1 模型训练，保存.h5格式的模型文件&lt;/h4&gt;

&lt;p&gt;在keras的&lt;code class=&quot;highlighter-rouge&quot;&gt;model.save()&lt;/code&gt;模型保存的函数中，只支持2种保存方式：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;.h5格式的文件进行保存模型整个结构及其权重。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;以文件夹（包含assets  saved_model.pb  variables）来保存，模型架构和训练配置（包括优化器、损失和指标）存储在 &lt;code class=&quot;highlighter-rouge&quot;&gt;saved_model.pb&lt;/code&gt; 中。权重保存在 &lt;code class=&quot;highlighter-rouge&quot;&gt;variables/&lt;/code&gt; 目录下。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;因此使用.h5格式来存储模型结构及权重。&lt;/p&gt;

&lt;h4 id=&quot;22-h5文件转pt文件&quot;&gt;2.2 h5文件转pt文件&lt;/h4&gt;

&lt;p&gt;将训练好的模型以.h5格式进行存储，再通过转成.pt格式的模型文件，提供C++的tensorflow调用模型。&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;keras.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_model&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow.python.keras.backend&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow.python.framework&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph_io&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 针对tf2.x来说不支持freezegraph的，这里需要使用tf1的方式&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;disable_eager_execution&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;freeze_session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keep_var_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clear_devices&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow.python.framework.graph_util&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;convert_variables_to_constants&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;freeze_var_names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;difference&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;keep_var_names&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]))&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;output_names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_names&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;output_names&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;input_graph_def&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_graph_def&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;clear_devices&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_graph_def&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
                &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;device&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;frozen_graph&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;convert_variables_to_constants&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_graph_def&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                                      &lt;span class=&quot;n&quot;&gt;output_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freeze_var_names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frozen_graph&lt;/span&gt;


&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;----------------------------------配置路径-----------------------------------&quot;&quot;&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;h5_model_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'model/model.h5'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;pb_model_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'model.pt'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;output_path&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'.'&lt;/span&gt;

&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;----------------------------------导入keras模型------------------------------&quot;&quot;&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set_learning_phase&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;net_model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;load_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h5_model_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'input is :'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;input&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'output is:'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;----------------------------------保存为.pb格式------------------------------&quot;&quot;&quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;frozen_graph&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;freeze_session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;K&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_names&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;op&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;net_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;graph_io&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;write_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frozen_graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;output_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pb_model_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;as_text&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;23-测试pt模型文件&quot;&gt;2.3 测试pt模型文件&lt;/h4&gt;

&lt;p&gt;对同一个图片分别利用.h5模型文件和.pt模型文件进行预测。.pt格式的模型预测代码如下：&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;pred_with_pt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pb_file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Graph&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as_default&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;():&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;output_graph_def&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;GraphDef&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;# 打开.pb模型&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pb_file_path&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;rb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;output_graph_def&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ParseFromString&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;tensors&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;import_graph_def&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;output_graph_def&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;tensors:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tensors&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;c&quot;&gt;# 在一个session中去run一个前向&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;init&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;global_variables_initializer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;init&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;n&quot;&gt;op&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_operations&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

            &lt;span class=&quot;n&quot;&gt;input_x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_tensor_by_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;conv2d_input:0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# 具体名称看上一段代码的input.name&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;input_X:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;input_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;n&quot;&gt;out_softmax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;graph&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get_tensor_by_name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;dense/Softmax:0&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c&quot;&gt;# 具体名称看上一段代码的output.name&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;Output:&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out_softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

            &lt;span class=&quot;n&quot;&gt;img_out_softmax&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sess&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;run&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out_softmax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                       &lt;span class=&quot;n&quot;&gt;feed_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;input_x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;

            &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;img_out_softmax&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;三-c-模型预测&quot;&gt;三. C++ 模型预测&lt;/h3&gt;

&lt;h4 id=&quot;31-将模型预测类进行封装并生成动态链接库dll&quot;&gt;3.1 将模型预测类进行封装并生成动态链接库DLL&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;VS2019部署（参考本人的上一篇博客）：新建一个动态链接库(DLL)，新建一个头文件&lt;code class=&quot;highlighter-rouge&quot;&gt;tf_clf.h&lt;/code&gt;和源文件&lt;code class=&quot;highlighter-rouge&quot;&gt;tf_clf.cpp&lt;/code&gt;，这里是生成DLL包（点击&lt;code class=&quot;highlighter-rouge&quot;&gt;生成&lt;/code&gt; ==&amp;gt; &lt;code class=&quot;highlighter-rouge&quot;&gt;重新生成DLLTF&lt;/code&gt;）&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;这里要把&lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow_cc.dll&lt;/code&gt;放到生成的&lt;code class=&quot;highlighter-rouge&quot;&gt;x64/release&lt;/code&gt;里面&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;这里还需要在Release属性页里配置&lt;code class=&quot;highlighter-rouge&quot;&gt;C/C++&lt;/code&gt;的预处理器(防止后面编译时出现这种错误&lt;code class=&quot;highlighter-rouge&quot;&gt;tstring.h(350,40): error C2589: “(”:“::”&lt;/code&gt;)：&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;_XKEYCHECK_H
NOMINMAX
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;头文件&lt;code class=&quot;highlighter-rouge&quot;&gt;tf_clf.h&lt;/code&gt;中声明类的导出&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class __declspec(dllexport) TFClf;
class TFClf {
private:
vector&amp;lt;float&amp;gt; mean = { 103.939,116.779,123.68 };
int resize_col = 224;
int resize_row = 224;
string input_tensor_name = &quot;conv2d_input&quot;;
string output_tensor_name = &quot;dense/Softmax&quot;;
Point draw_point = Point(50, 50);

public:
string image_path, model_path;
TFClf(string img, string model) :image_path(img), model_path(model) {}
void mat_to_tensor(Mat img, Tensor* output_tensor);
Mat preprocess_img(Mat img);
void model_pred();
void show_result_pic(Mat img, int output_class_id, double output_prob);
};
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;源文件&lt;code class=&quot;highlighter-rouge&quot;&gt;tf_clf.cpp&lt;/code&gt;完成类的具体实现，注意这里要&lt;code class=&quot;highlighter-rouge&quot;&gt;#include &quot;pch.h&quot;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;32-新建一个项目测试dll&quot;&gt;3.2 新建一个项目测试DLL&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;新建一个控制台的空项目，并将打包好的&lt;code class=&quot;highlighter-rouge&quot;&gt;DllTF.dll&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;DllTF.lib&lt;/code&gt;复制到工程中&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;配置属性管理器：这里需要在&lt;code class=&quot;highlighter-rouge&quot;&gt;Release | x64&lt;/code&gt;添加之前配置好的&lt;code class=&quot;highlighter-rouge&quot;&gt;opencv_release.props&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;tf_release.props&lt;/code&gt;。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;这里要把&lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow_cc.dll&lt;/code&gt;放到生成的&lt;code class=&quot;highlighter-rouge&quot;&gt;x64/release&lt;/code&gt;里面&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;新建一个头文件&lt;code class=&quot;highlighter-rouge&quot;&gt;tf_clf.h&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;#pragma once
#ifndef TF_CLF_H

#endif // !TF_CLF_H

#pragma comment(lib,&quot;DllTF.lib&quot;)
class __declspec(dllexport) TFClf;


class TFClf {
private:
    vector&amp;lt;float&amp;gt; mean = { 103.939,116.779,123.68 };
    int resize_col = 224;
    int resize_row = 224;
    string input_tensor_name = &quot;conv2d_input&quot;;
    string output_tensor_name = &quot;dense/Softmax&quot;;
    Point draw_point = Point(50, 50);

public:
    string image_path, model_path;
    TFClf(string img, string model) :image_path(img), model_path(model) {}
    void mat_to_tensor(Mat img, Tensor* output_tensor);
    Mat preprocess_img(Mat img);
    void model_pred();
    void show_result_pic(Mat img, int output_class_id, double output_prob);
};
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;新建一个源文件&lt;code class=&quot;highlighter-rouge&quot;&gt;main.cpp&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;# include &quot;tf_clf.h&quot;

int main() {
string model_path = &quot;D:/yeyan/pycharm_project/dogcat/model/model.pt&quot;;
string img_path = &quot;D:/yeyan/pycharm_project/dogcat/data/test_set/test_set/cats/cat.4001.jpg&quot;;
TFClf clf = TFClf(img_path, model_path);
clf.model_pred();
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content>

      
      
      
      
      

      <author>
          <name></name>
        
        
      </author>

      

      
        <category term="keras" />
      
        <category term="DeepLearning" />
      
        <category term="环境搭建" />
      

      
        <summary type="html">主要讲述将Keras训练好的.h5模型转换成.pt格式，并将其转换后的模型部署到C++环境中，并生成DLL</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Window下搭建Tensorflow的C++环境</title>
      <link href="http://localhost:4000/Window%E4%B8%8B%E6%90%AD%E5%BB%BATensorflow%E7%9A%84C++%E7%8E%AF%E5%A2%83" rel="alternate" type="text/html" title="Window下搭建Tensorflow的C++环境" />
      <published>2020-11-02T03:21:00+08:00</published>
      <updated>2020-11-02T03:21:00+08:00</updated>
      <id>http://localhost:4000/Window%E4%B8%8B%E6%90%AD%E5%BB%BATensorflow%E7%9A%84C++%E7%8E%AF%E5%A2%83</id>
      <content type="html" xml:base="http://localhost:4000/Window%E4%B8%8B%E6%90%AD%E5%BB%BATensorflow%E7%9A%84C++%E7%8E%AF%E5%A2%83">&lt;blockquote&gt;
  &lt;p&gt;参考Tensorflow官网安装文章：https://www.tensorflow.org/install/source_windows?hl=zh-cn&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;一-下载需要的软件&quot;&gt;一. 下载需要的软件&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;bazel：Google 的一款可再生的代码构建工具，类似于Cmake。使用scoop进行安装：&lt;code class=&quot;highlighter-rouge&quot;&gt;scoop install bazel&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;python3.7：这里最好用pip 安装下必要的第三方包，比如&lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow,kears,numpy&lt;/code&gt;等。&lt;/li&gt;
  &lt;li&gt;下载官方源码：&lt;code class=&quot;highlighter-rouge&quot;&gt;git clone https://github.com/tensorflow/tensorflow.git&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;二-进行bazel源码编译&quot;&gt;二. 进行bazel源码编译&lt;/h3&gt;

&lt;h4 id=&quot;21-配置build&quot;&gt;2.1 配置build&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;cd到源码目录：&lt;code class=&quot;highlighter-rouge&quot;&gt;cd tensorflow-master&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;通过在 TensorFlow 源代码树的根目录下运行以下命令来配置系统构建：&lt;code class=&quot;highlighter-rouge&quot;&gt;python3 ./configure.py&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;这里选择的是cpu版本的，每个配置的选择如下：&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;You have bazel 3.7.0 installed.
Please specify the location of python. &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Default is C:&lt;span class=&quot;se&quot;&gt;\s&lt;/span&gt;oft&lt;span class=&quot;se&quot;&gt;\p&lt;/span&gt;ython3.7.9&lt;span class=&quot;se&quot;&gt;\p&lt;/span&gt;ython3.exe]:


Found possible Python library paths:
  C:&lt;span class=&quot;se&quot;&gt;\s&lt;/span&gt;oft&lt;span class=&quot;se&quot;&gt;\p&lt;/span&gt;ython3.7.9&lt;span class=&quot;se&quot;&gt;\l&lt;/span&gt;ib&lt;span class=&quot;se&quot;&gt;\s&lt;/span&gt;ite-packages
Please input the desired Python library path to use.  Default is &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;C:&lt;span class=&quot;se&quot;&gt;\s&lt;/span&gt;oft&lt;span class=&quot;se&quot;&gt;\p&lt;/span&gt;ython3.7.9&lt;span class=&quot;se&quot;&gt;\l&lt;/span&gt;ib&lt;span class=&quot;se&quot;&gt;\s&lt;/span&gt;ite-packages]

Do you wish to build TensorFlow with ROCm support? &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;y/N]: n

Do you wish to build TensorFlow with CUDA support? &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;y/N]: n
No CUDA support will be enabled &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;TensorFlow.

Please specify optimization flags to use during compilation when bazel option &lt;span class=&quot;s2&quot;&gt;&quot;--config=opt&quot;&lt;/span&gt; is specified &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Default is /arch:AVX]:


Would you like to override eigen strong inline &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;some C++ compilation to reduce the compilation &lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt;? &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Y/n]: y
Eigen strong inline overridden.

Would you like to interactively configure ./WORKSPACE &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;Android builds? &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;y/N]: n
Not configuring the WORKSPACE &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;Android builds.

Preconfigured Bazel build configs. You can use any of the below by adding &lt;span class=&quot;s2&quot;&gt;&quot;--config=&amp;lt;&amp;gt;&quot;&lt;/span&gt; to your build command. See .bazelrc &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;more details.
        &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;mkl            &lt;span class=&quot;c&quot;&gt;# Build with MKL support.&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;mkl_aarch64    &lt;span class=&quot;c&quot;&gt;# Build with oneDNN support for Aarch64.&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;monolithic     &lt;span class=&quot;c&quot;&gt;# Config for mostly static monolithic build.&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;numa           &lt;span class=&quot;c&quot;&gt;# Build with NUMA support.&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;dynamic_kernels        &lt;span class=&quot;c&quot;&gt;# (Experimental) Build kernels into separate shared objects.&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;v2             &lt;span class=&quot;c&quot;&gt;# Build TensorFlow 2.x instead of 1.x.&lt;/span&gt;
Preconfigured Bazel build configs to DISABLE default on features:
        &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;noaws          &lt;span class=&quot;c&quot;&gt;# Disable AWS S3 filesystem support.&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;nogcp          &lt;span class=&quot;c&quot;&gt;# Disable GCP support.&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;nohdfs         &lt;span class=&quot;c&quot;&gt;# Disable HDFS support.&lt;/span&gt;
        &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;nonccl         &lt;span class=&quot;c&quot;&gt;# Disable NVIDIA NCCL support.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;22-bazel编译&quot;&gt;2.2 bazel编译&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;修改bazel中间文件存储的路径（&lt;strong&gt;磁盘可用空间 Release 版本 &amp;gt;= 16G , Debug版本 &amp;gt;= 40G&lt;/strong&gt; 编译的中间文件默认会放到 &lt;strong&gt;C:\用户\你的账号名\ _bazel_你的账号名&lt;/strong&gt; 下. C 盘可能没有那么大的空间, 所以要改一下输出文件的路径），打开tensorflow文件夹，&lt;code class=&quot;highlighter-rouge&quot;&gt;vim .bazelrc&lt;/code&gt;，在最后一行加上&lt;code class=&quot;highlighter-rouge&quot;&gt;startup --output_user_root=D:/tf&lt;/code&gt;，如果不修改路径，可能会编译到一半就卡死。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;bazel编译动态链接库命令（这里加上使用的最大内存）：
    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; bazel build &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;opt //tensorflow:tensorflow_cc.dll &lt;span class=&quot;nt&quot;&gt;--local_ram_resources&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1024
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;编译的过程可能会很长，千万不要以为有问题就&lt;code class=&quot;highlighter-rouge&quot;&gt;Ctrl C&lt;/code&gt;了（分2个过程：下中间资源+编译），编译完成后会出现&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; Build completed successfully
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;编译好的库文件在&lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow-master\bazel-bin\tensorflow&lt;/code&gt;目录下，分别是&lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow_cc.dll&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow_cc.dll.if.lib&lt;/code&gt;。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;bazel编译头文件命令：&lt;/p&gt;

    &lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; bazel build &lt;span class=&quot;nt&quot;&gt;--config&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;opt //tensorflow:install_headers &lt;span class=&quot;nt&quot;&gt;--local_ram_resources&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1024
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;编译好的头文件在&lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow-master\bazel-bin\tensorflow\include&lt;/code&gt;目录下。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;三-新建项目测试&quot;&gt;三. 新建项目测试&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;注意：&lt;/p&gt;

  &lt;p&gt;​	1. 这里编译的是tensorflow的release版本，因此构建项目的时候把环境从debug变成release&lt;/p&gt;

  &lt;p&gt;​	2. 在新建项目属性表（这里无论是opencv还是tensorflow）中，要选择release版本的x64（64位）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;新建一个项目&lt;/li&gt;
  &lt;li&gt;在项目中新建一个文件夹存放之前编译好的头文件，库文件，具体结构如下所示&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-js highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;├──&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;tf_test&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;// 整个项目&lt;/span&gt;
	&lt;span class=&quot;err&quot;&gt;├──&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;x64&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// 这里是生成解决方案得到的&lt;/span&gt;
	&lt;span class=&quot;err&quot;&gt;├──&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;tf&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// 这里存放所有编译好的文件&lt;/span&gt;
    	&lt;span class=&quot;err&quot;&gt;├──&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;bin&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// 存放dll动态库文件&lt;/span&gt;
        	&lt;span class=&quot;err&quot;&gt;├──&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;tensorflow_cc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;dll&lt;/span&gt;
        &lt;span class=&quot;err&quot;&gt;├──&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lib&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// 存放静态库文件&lt;/span&gt;
        	&lt;span class=&quot;err&quot;&gt;├──&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;tensorflow_cc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lib&lt;/span&gt;
        &lt;span class=&quot;err&quot;&gt;├──&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;include&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;// 直接是tensorflow编译好的include目录&lt;/span&gt;
    &lt;span class=&quot;err&quot;&gt;├──&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;cpp&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;属性管理器 —— Release X64 —— 添加新项目属性表（如果代码中还需要添加opencv库的可以参考本人&lt;a href=&quot;https://yy2lyx.github.io/Visual-Studio-2019-%E4%B8%8B%E6%90%AD%E5%BB%BAopencv3.4.11%E7%9A%84C++%E7%8E%AF%E5%A2%83/&quot;&gt;另一篇博客&lt;/a&gt;）&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;VC++目录中的包含目录中添加：&lt;code class=&quot;highlighter-rouge&quot;&gt;D:tf_test\tf\include&lt;/code&gt;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;VC++目录中的库目录中添加：&lt;code class=&quot;highlighter-rouge&quot;&gt;D:tf_test\tf\lib&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;链接器——输入——附加依赖项中添加：&lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow_cc.lib&lt;/code&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;选择项目为release和x64平台。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;使用以下代码进行测试&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&quot;language-C++&quot;&gt;#include &amp;lt;iostream&amp;gt;
#include &amp;lt;opencv2/highgui/highgui.hpp&amp;gt;
#include&amp;lt;opencv2/opencv.hpp&amp;gt;
#include&quot;tensorflow/core/public/session.h&quot;
#include&quot;tensorflow/core/platform/env.h&quot;

using namespace std;
using namespace tensorflow;
using namespace cv;

int main()
{
    const string model_path = &quot;D:\\code\\yinbao_face\\live.pb&quot;;
    const string image_path = &quot;0.jpg&quot;;


    Mat img = imread(image_path);
    cvtColor(img, img, COLOR_BGR2RGB);
    resize(img, img, Size(112, 112), 0, 0, INTER_NEAREST);
    int height = img.rows;
    int width = img.cols;
    int depth = img.channels();

    // 图像预处理
    img = (img - 0) / 255.0;
   // img.convertTo(img, CV_32FC3, 1.0 / 255, 0);

    // 取图像数据，赋给tensorflow支持的Tensor变量中
    const float* source_data = (float*)img.data;
    Tensor input_tensor(DT_FLOAT, TensorShape({ 1, height, width, 3 }));
    auto input_tensor_mapped = input_tensor.tensor&amp;lt;float, 4&amp;gt;();

    for (int i = 0; i &amp;lt; height; i++) {
        const float* source_row = source_data + (i * width * depth);
        for (int j = 0; j &amp;lt; width; j++) {
            const float* source_pixel = source_row + (j * depth);
            for (int c = 0; c &amp;lt; depth; c++) {
                const float* source_value = source_pixel + c;
                input_tensor_mapped(0, i, j, c) = *source_value;
                //printf(&quot;%d&quot;);
            }
        }
    }

    Session* session;

    Status status = NewSession(SessionOptions(), &amp;amp;session);
    if (!status.ok()) {
        cerr &amp;lt;&amp;lt; status.ToString() &amp;lt;&amp;lt; endl;
        return -1;
    }
    else {
        cout &amp;lt;&amp;lt; &quot;Session created successfully&quot; &amp;lt;&amp;lt; endl;
    }
    GraphDef graph_def;
    Status status_load = ReadBinaryProto(Env::Default(), model_path, &amp;amp;graph_def);
    if (!status_load.ok()) {
        cerr &amp;lt;&amp;lt; status_load.ToString() &amp;lt;&amp;lt; endl;
        return -1;
    }
    else {
        cout &amp;lt;&amp;lt; &quot;Load graph protobuf successfully&quot; &amp;lt;&amp;lt; endl;
    }

    // 将graph加载到session
    Status status_create = session-&amp;gt;Create(graph_def);
    if (!status_create.ok()) {
        cerr &amp;lt;&amp;lt; status_create.ToString() &amp;lt;&amp;lt; endl;
        return -1;
    }
    else {
        cout &amp;lt;&amp;lt; &quot;Add graph to session successfully&quot; &amp;lt;&amp;lt; endl;
    }

    cout &amp;lt;&amp;lt; input_tensor.DebugString() &amp;lt;&amp;lt; endl; //打印输入
    vector&amp;lt;pair&amp;lt;string, Tensor&amp;gt;&amp;gt; inputs = {
        { &quot;input_1:0&quot;, input_tensor },  //input_1:0为输入节点名
    };

    // 输出outputs
    vector&amp;lt;Tensor&amp;gt; outputs;
    vector&amp;lt;string&amp;gt; output_nodes;
    output_nodes.push_back(&quot;output_1:0&quot;);  //输出有多个节点的话就继续push_back

    double start = clock();
    // 运行会话，最终结果保存在outputs中
    Status status_run = session-&amp;gt;Run({ inputs }, { output_nodes }, {}, &amp;amp;outputs);
    Tensor boxes = move(outputs.at(0));
    cout &amp;lt;&amp;lt; boxes.DebugString() &amp;lt;&amp;lt; endl; //打印输出

    double end = clock();
    cout &amp;lt;&amp;lt; &quot;time = &quot; &amp;lt;&amp;lt; (end - start) &amp;lt;&amp;lt; &quot;\n&quot;;
    if (!status_run.ok()) {
        cerr &amp;lt;&amp;lt; status_run.ToString() &amp;lt;&amp;lt; endl;
        return -1;
    }
    else {
        //cout &amp;lt;&amp;lt; &quot;Run session successfully&quot; &amp;lt;&amp;lt; endl;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&quot;四-测试中出现的问题&quot;&gt;四. 测试中出现的问题&lt;/h3&gt;

&lt;h4 id=&quot;41-生成解决方案的时候报错无法打开包括文件&quot;&gt;4.1 生成解决方案的时候报错无法打开包括文件：&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://tva1.sinaimg.cn/large/0081Kckwgy1gkelmo3blhj30gz01ia9t.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;解决方式：在本地的通过python pip安装后的tensorflow文件夹中（&lt;code class=&quot;highlighter-rouge&quot;&gt;C:\soft\python3.7.9\Lib\site-packages\tensorflow\include&lt;/code&gt;）将&lt;code class=&quot;highlighter-rouge&quot;&gt;google&lt;/code&gt;文件夹复制到&lt;code class=&quot;highlighter-rouge&quot;&gt;D:tf_test\tf\include&lt;/code&gt;下面，即可解决&lt;/p&gt;

&lt;h4 id=&quot;42-生成解决方案的时候报错link1120&quot;&gt;4.2 生成解决方案的时候报错Link1120:&lt;/h4&gt;

&lt;p&gt;&lt;img src=&quot;https://tva1.sinaimg.cn/large/0081Kckwgy1gkeln9lgpwj30zt01owec.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;解决方式：将vs2019上报错信息复制，cd到&lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow-master\tensorflow\tools\def_file_filter&lt;/code&gt;(这里的tensorflow-master是自己下载tensorflow源码的地方），编辑&lt;code class=&quot;highlighter-rouge&quot;&gt;def_file_filter.py.tpl&lt;/code&gt;文件：&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Header for the def file. (找到这一行代码)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if &lt;/span&gt;args.target:
    def_fp.write&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;LIBRARY &quot;&lt;/span&gt; + args.target + &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    def_fp.write&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;EXPORTS&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    def_fp.write&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; ??1OpDef@tensorflow@@UEAA@XZ&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 下面两个就是复制的错误信息&lt;/span&gt;
    def_fp.write&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; ?NewSession@tensorflow@@YA?AVStatus@1@AEBUSessionOptions@1@PEAPEAVSession@1@@Z&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    def_fp.write&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; ??0SessionOptions@tensorflow@@QEAA@XZ&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;重新编译DLL，头文件（虽然很麻烦，但是还是得做啊）&lt;/p&gt;

&lt;h4 id=&quot;43-有太多的错误导致intellisense引擎无法正常工作其中有些错误无法在编辑器&quot;&gt;4.3 有太多的错误导致IntelliSense引擎无法正常工作,其中有些错误无法在编辑器&lt;/h4&gt;

&lt;p&gt;解决方式：在项目-&amp;gt;属性-&amp;gt;配置属性-&amp;gt;C/C++-&amp;gt;预处理器-&amp;gt;预处理器定义中加入&lt;code class=&quot;highlighter-rouge&quot;&gt;_XKEYCHECK_H&lt;/code&gt;就消失了&lt;/p&gt;

&lt;h4 id=&quot;44-找不到tensorflow_ccdll文件&quot;&gt;4.4 找不到tensorflow_cc.dll文件&lt;/h4&gt;

&lt;p&gt;解决方式：将&lt;code class=&quot;highlighter-rouge&quot;&gt;tensorflow_cc.dll&lt;/code&gt;文件复制到&lt;code class=&quot;highlighter-rouge&quot;&gt;x64/release&lt;/code&gt;文件夹下。&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name></name>
        
        
      </author>

      

      
        <category term="环境搭建" />
      
        <category term="Tensorflow" />
      
        <category term="DeepLearning" />
      

      
        <summary type="html">介绍如何在windows下搭建tensorflow的C++环境</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">pytorch使用YOLOv5训练数据</title>
      <link href="http://localhost:4000/pytorch%E4%BD%BF%E7%94%A8YOLOv5%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE" rel="alternate" type="text/html" title="pytorch使用YOLOv5训练数据" />
      <published>2020-10-27T03:21:00+08:00</published>
      <updated>2020-10-27T03:21:00+08:00</updated>
      <id>http://localhost:4000/pytorch%E4%BD%BF%E7%94%A8YOLOv5%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE</id>
      <content type="html" xml:base="http://localhost:4000/pytorch%E4%BD%BF%E7%94%A8YOLOv5%E8%AE%AD%E7%BB%83%E6%95%B0%E6%8D%AE">&lt;h3 id=&quot;一-需要下载的资源&quot;&gt;一. 需要下载的资源&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Fork 下官方的开源项目：https://github.com/ultralytics/yolov5&lt;/li&gt;
  &lt;li&gt;git clone 下Fork之后的项目到自己本地仓库中。&lt;/li&gt;
  &lt;li&gt;采用的训练集（简单的，仅有一个类）：源自&lt;a href=&quot;https://www.kaggle.com/c/global-wheat-detection/data&quot;&gt;Kaggle的小麦数据集&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;如果有gpu的话，最好安装cuda进行训练加速，这里可以参考本人的另一篇文章：&lt;a href=&quot;https://yy2lyx.github.io/Windows10%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%90%AD%E5%BB%BACUDA10.1%E5%92%8Cpytorch1.6/&quot;&gt;Windows10环境下搭建CUDA10.1和pytorch1.6&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;二--构建属于自己的目标检测模型&quot;&gt;二 . 构建属于自己的目标检测模型&lt;/h3&gt;

&lt;h4 id=&quot;21-在官网的开源yolo5项目的基础上进行构建&quot;&gt;2.1 在官网的开源yolo5项目的基础上进行构建&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;git 克隆到本地仓库：&lt;code class=&quot;highlighter-rouge&quot;&gt;git clone https://github.com/xx/yolov5.git&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;进入项目中，并安装需要的第三方依赖：&lt;code class=&quot;highlighter-rouge&quot;&gt;pip install -r requirements.txt&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;新建一个原始数据的目录：：&lt;code class=&quot;highlighter-rouge&quot;&gt;mkdir ori_data&lt;/code&gt;，将下载好的小麦数据集解压后放到项目。&lt;/li&gt;
  &lt;li&gt;创建输出一个文件输出目录：&lt;code class=&quot;highlighter-rouge&quot;&gt;mkdir wheat_data&lt;/code&gt;，并在此目录下新建以下目录，如下图所示
    &lt;div class=&quot;language-js highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;err&quot;&gt;├─&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;images&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;│&lt;/span&gt;  &lt;span class=&quot;err&quot;&gt;├─&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;train&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;│&lt;/span&gt;  &lt;span class=&quot;err&quot;&gt;└─&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;val&lt;/span&gt;
&lt;span class=&quot;err&quot;&gt;└─&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;labels&lt;/span&gt;
  &lt;span class=&quot;err&quot;&gt;├─&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;train&lt;/span&gt;
  &lt;span class=&quot;err&quot;&gt;└─&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;val&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;构建数据集，新建一个&lt;code class=&quot;highlighter-rouge&quot;&gt;munge_data.py&lt;/code&gt;文件&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;ast&lt;/span&gt; 
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.model_selection&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tqdm&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;shutil&lt;/span&gt;


&lt;span class=&quot;n&quot;&gt;DATA_PATH&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ori_data'&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;OUTPUT_PATH&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'wheat_data'&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;process_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_type&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'train'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tqdm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iterrows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;image_name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'image_id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;bounding_boxes&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'bbox'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;yolo_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bbox&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bounding_boxes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bbox&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bbox&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bbox&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bbox&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;x_center&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;y_center&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
            &lt;span class=&quot;c&quot;&gt;# 这里需要将图像数据归一化处理（yolo需要的输入为归一化后的数据,且为浮点数）&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;x_center&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1024.0&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;y_center&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1024.0&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;w&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1024.0&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;h&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;1024.0&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;yolo_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x_center&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y_center&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;w&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;h&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;yolo_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;array&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yolo_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# 保存bbox的图片信息&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savetxt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;OUTPUT_PATH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'labels/{data_type}/{image_name}.txt'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;yolo_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;fmt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;d'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;f'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;c&quot;&gt;# 将目标图片文件保存到指定文件中&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;shutil&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;copyfile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DATA_PATH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'train/{image_name}.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;OUTPUT_PATH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'images/{data_type}/{image_name}.jpg'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;



&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__name__&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;__main__&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;join&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DATA_PATH&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'train.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 将string of list 转成list数据&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bbox&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bbox&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ast&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;literal_eval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 利用groupby 将同一个image_id的数据进行聚合，方式为list进行，并且用reset_index直接转变成dataframe&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'image_id'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'bbox'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;apply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'bbox'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;c&quot;&gt;# 划分数据集&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;42&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;c&quot;&gt;# 重设 index （这里数据被打乱，index改变混乱）&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df_train&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df_train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df_val&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df_val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;process_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'train'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;process_data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'val'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;运行构建数据的py文件：&lt;code class=&quot;highlighter-rouge&quot;&gt;python munge_data.py&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;这里可以看到在输出结果目录中，放入了需要的整理后的数据集&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;新建一个&lt;code class=&quot;highlighter-rouge&quot;&gt;wheat.yaml&lt;/code&gt;yaml文件，指定模型训练时候的输入及其类别（注意这里冒号后面要加空格，yamal格式问题）&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;wheat_data/images/train // 指定训练目录&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;val&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;wheat_data/images/val // 指定验证目录&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;nc&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;1 // 指定类别&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;pi&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;wheat&quot;&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;]&lt;/span&gt;  &lt;span class=&quot;s&quot;&gt;// 指定类别名字&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;进行模型训练：&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python3 train.py &lt;span class=&quot;nt&quot;&gt;--img&lt;/span&gt; 1024 &lt;span class=&quot;nt&quot;&gt;--batch&lt;/span&gt; 8 &lt;span class=&quot;nt&quot;&gt;--epoch&lt;/span&gt; 100 &lt;span class=&quot;nt&quot;&gt;--data&lt;/span&gt; wheat.yaml &lt;span class=&quot;nt&quot;&gt;--cfg&lt;/span&gt; .&lt;span class=&quot;se&quot;&gt;\m&lt;/span&gt;odels&lt;span class=&quot;se&quot;&gt;\y&lt;/span&gt;olov5s.yaml &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; wm
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;这里可能会报错（Dataloader中设置了多进程导致的），报错信息如下所示：&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;File &lt;span class=&quot;s2&quot;&gt;&quot;C:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\s&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;oft&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\p&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;ython3.7.9&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\l&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;ib&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\m&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;ultiprocessing&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\r&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;eduction.py&quot;&lt;/span&gt;, line 60, &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;dump     
ForkingPickler&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;file, protocol&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;.dump&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;obj&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; BrokenPipeError: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Errno 32] Broken pipe
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;这里可以参考文章：https://github.com/pytorch/pytorch/issues/2341。解决方案：将&lt;code class=&quot;highlighter-rouge&quot;&gt;utils\datasets.py&lt;/code&gt;文件中&lt;code class=&quot;highlighter-rouge&quot;&gt;num_workers&lt;/code&gt;改成0即可（代码第68行）。训练完成后如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://tva1.sinaimg.cn/large/0081Kckwgy1gk47ziddqvj30jg01vt8i.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;可以查看本地tensorboard训练过程：&lt;code class=&quot;highlighter-rouge&quot;&gt;tensorboard --logdir=runs&lt;/code&gt;，如果这里出现问题，可以换成一下命令：&lt;code class=&quot;highlighter-rouge&quot;&gt;python -m tensorboard.main --logdir logs&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://tva1.sinaimg.cn/large/0081Kckwgy1gk4804vrrkj310d0gajt5.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;这里还可以使用coco数据集的&lt;a href=&quot;https://drive.google.com/drive/folders/1Drs_Aiu7xx6S-ix95f9kNsA6ueKRpN2J&quot;&gt;预训练模型&lt;/a&gt;进行训练，可能效果会更好&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;python3 train.py &lt;span class=&quot;nt&quot;&gt;--img&lt;/span&gt; 1024 &lt;span class=&quot;nt&quot;&gt;--batch&lt;/span&gt; 8 &lt;span class=&quot;nt&quot;&gt;--epoch&lt;/span&gt; 100 &lt;span class=&quot;nt&quot;&gt;--data&lt;/span&gt; wheat.yaml &lt;span class=&quot;nt&quot;&gt;--cfg&lt;/span&gt; .&lt;span class=&quot;se&quot;&gt;\m&lt;/span&gt;odels&lt;span class=&quot;se&quot;&gt;\y&lt;/span&gt;olov5s.yaml &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; wm &lt;span class=&quot;nt&quot;&gt;--weights&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;将训练好的模型放到当前文件夹下：&lt;code class=&quot;highlighter-rouge&quot;&gt;cp runs/exp0_wm/weights/best.pt . &lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;选择测试图片的文件夹进行生成测试：&lt;code class=&quot;highlighter-rouge&quot;&gt;python detect.py --source ./test_data --weights best.pt &lt;/code&gt;，这里可以看到新生成一个文件夹&lt;code class=&quot;highlighter-rouge&quot;&gt;inference/output&lt;/code&gt;中就是测试后标记bbox后的图片。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name></name>
        
        
      </author>

      

      
        <category term="pytorch" />
      
        <category term="ComputerVision" />
      
        <category term="DeepLearning" />
      

      
        <summary type="html">介绍如何利用官方开源的Yolov5训练一个属于自己的模型</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Windows10环境下搭建CUDA10.1和pytorch1.6</title>
      <link href="http://localhost:4000/Windows10%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%90%AD%E5%BB%BACUDA10.1%E5%92%8Cpytorch1.6" rel="alternate" type="text/html" title="Windows10环境下搭建CUDA10.1和pytorch1.6" />
      <published>2020-10-20T18:18:00+08:00</published>
      <updated>2020-10-20T18:18:00+08:00</updated>
      <id>http://localhost:4000/Windows10%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%90%AD%E5%BB%BACUDA10.1%E5%92%8Cpytorch1.6</id>
      <content type="html" xml:base="http://localhost:4000/Windows10%E7%8E%AF%E5%A2%83%E4%B8%8B%E6%90%AD%E5%BB%BACUDA10.1%E5%92%8Cpytorch1.6">&lt;h3 id=&quot;一-安装cuda101&quot;&gt;一. 安装CUDA10.1&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;这里需要安装鲁大师，查看自己电脑的显卡型号，这里是gtx 1060 ，6g&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;11-安装nvidia驱动&quot;&gt;1.1 安装Nvidia驱动&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;首先在Nvidia官网上安装显卡驱动，连接地址：https://www.nvidia.com/Download/index.aspx&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://tva1.sinaimg.cn/large/007S8ZIlgy1gjx7vzmyjnj30io0ak3zx.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;这里需要去nVidia官网 https://docs.nvidia.com/cuda/cuda-toolkit-release-notes/index.html 查看cuda和显卡驱动对应表上cuda10.1对应驱动的版本号。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://tva1.sinaimg.cn/large/007S8ZIlgy1gjx7wolh48j30n80ctmxv.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;查看自己安装驱动的版本号：NVIDIA控制面板-帮助-系统信息中查看，这里可以看到我的驱动的版本号为456，这里是大于官网安装cuda10.1需要的418的驱动的。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://tva1.sinaimg.cn/large/007S8ZIlgy1gjx7wcmibaj30qh0jh3z7.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;12-安装cuda101&quot;&gt;1.2 安装CUDA10.1&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;安装CUDA：适合哪个版本的CUDA，就可以去官网下载对应的CUDA了，但是官网首页的CUDA一般是最新版，我们可能需要下载旧版本比如CUDA10.1，可以cuu点击下面连接进行下载：https://developer.nvidia.com/cuda-toolkit-archiv，这里直接选择&lt;a href=&quot;https://developer.nvidia.com/cuda-10.1-download-archive-base&quot;&gt;CUDA Toolkit 10.1 &lt;/a&gt;(Feb 2019)即可。这里一般选择自定义安装，可以通过命令行查看是否安装成功：&lt;code class=&quot;highlighter-rouge&quot;&gt;nvcc -V&lt;/code&gt;，这里如果提升没有整个命令，说明还没有将cuda的路径添加到环境变量中，需要设置环境变量，添加CUDA安装目录下的&lt;code class=&quot;highlighter-rouge&quot;&gt;bin&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;libnvvp&lt;/code&gt;目录，如下所示：&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://tva1.sinaimg.cn/large/007S8ZIlgy1gjx7xf2l38j30jx09e74w.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;安装cuDNN：选择对应的版本号和系统，这里注意官网最前面的几个连接中都是windows10 的x86（32位）的，这里需要选择老一点的cuDNN的版本，如下所示&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://tva1.sinaimg.cn/large/007S8ZIlgy1gjx7ww4z84j30rl0g9t9n.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;下载后，将压缩包解压得到cuda文件夹，文件夹下有三个文件夹，复制这三个文件夹到CUDA安装的目录’D:\soft\cuda’下，会自动将cudnn的三个文件夹的文件合并到其三个同名文件夹&lt;code class=&quot;highlighter-rouge&quot;&gt;bin&lt;/code&gt;、&lt;code class=&quot;highlighter-rouge&quot;&gt;include&lt;/code&gt;和&lt;code class=&quot;highlighter-rouge&quot;&gt;lib&lt;/code&gt;中。&lt;/li&gt;
  &lt;li&gt;查看自己电脑中Nvidia的GPU信息：&lt;code class=&quot;highlighter-rouge&quot;&gt;nvidia-smi&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;二-安装pytorch&quot;&gt;二. 安装pytorch&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;pytorch官网安装地址：https://pytorch.org/get-started/locally/&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;21-修改pip源提升下载包速度&quot;&gt;2.1 修改pip源，提升下载包速度&lt;/h4&gt;

&lt;p&gt;如果本地pip下载很慢，修改pip源：Linux下，修改 ~/.pip/pip.conf (没有就创建一个文件夹及文件。文件夹要加“.”，表示是隐藏文件夹)&lt;/p&gt;

&lt;p&gt;内容如下：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[global]
index-url = https://pypi.tuna.tsinghua.edu.cn/simple
[install]
trusted-host=mirrors.aliyun.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;windows下，直接在user目录中创建一个pip目录，如：C:\Users\xx\pip，新建文件pip.ini。内容同上。&lt;/p&gt;

&lt;h4 id=&quot;22-通过pip安装pytorch&quot;&gt;2.2 通过pip安装pytorch&lt;/h4&gt;
&lt;p&gt;这里可以直接访问&lt;a href=&quot;&quot;&gt;pytorch官网&lt;/a&gt;来选择适合自己的版本：&lt;/p&gt;

&lt;p&gt;pip安装：&lt;code class=&quot;highlighter-rouge&quot;&gt;pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html&lt;/code&gt;&lt;/p&gt;

&lt;h4 id=&quot;23-查看是否安装成功并查看能否驱动cuda&quot;&gt;2.3 查看是否安装成功，并查看能否驱动cuda&lt;/h4&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cuda&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;is_available&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content>

      
      
      
      
      

      <author>
          <name></name>
        
        
      </author>

      

      
        <category term="DeepLearning" />
      
        <category term="pytorch" />
      

      
        <summary type="html">介绍如何在Windows10环境中搭建GPU使用环境和pytorch</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">面试总结</title>
      <link href="http://localhost:4000/%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93" rel="alternate" type="text/html" title="面试总结" />
      <published>2020-06-12T23:21:00+08:00</published>
      <updated>2020-06-12T23:21:00+08:00</updated>
      <id>http://localhost:4000/%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93</id>
      <content type="html" xml:base="http://localhost:4000/%E9%9D%A2%E8%AF%95%E6%80%BB%E7%BB%93">&lt;blockquote&gt;
  &lt;p&gt;面试官会根据自己简历中提到的一些点进行提问，这里先自己对某些点进行深挖。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;一数据处理&quot;&gt;一.数据处理&lt;/h3&gt;
&lt;p&gt;海量数据：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;（1）数据量太大，无法短时间内处理完成&lt;/li&gt;
  &lt;li&gt;（2）无法一次性将数据放入内存中。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;11-缺失值处理&quot;&gt;1.1 缺失值处理&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;填充固定值：选取某个固定值/默认值填充缺失值。&lt;/li&gt;
  &lt;li&gt;填充均值：对每一列的缺失值，填充当列的均值。&lt;/li&gt;
  &lt;li&gt;填充中位数：对每一列的缺失值，填充当列的中位数。&lt;/li&gt;
  &lt;li&gt;填充众数：对每一列的缺失值，填充当列的众数。由于存在某列缺失值过多，众数为nan的情况，因此这里取的是每列删除掉nan值后的众数。&lt;/li&gt;
  &lt;li&gt;填充上下条的数据：对每一条数据的缺失值，填充其上下条数据的值。&lt;/li&gt;
  &lt;li&gt;填充插值得到的数据：用插值法拟合出缺失的数据，然后进行填充。插值是离散函数逼近的重要方法，利用它可通过函数在有限个点处的取值状况，估算出函数在其他点处的近似值。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;二机器学习&quot;&gt;二.机器学习&lt;/h3&gt;
&lt;h4 id=&quot;21--svm和lr的区别与联系&quot;&gt;2.1  SVM和LR的区别与联系？&lt;/h4&gt;
&lt;p&gt;SVM 和 LR 都是属于分类算法，不过 SVM 是通过划分超平面的方法来进行分类，而 LR 则是通过计算样本属于哪个类别的概率，从而达到分类效果&lt;/p&gt;

&lt;h4 id=&quot;22--交叉熵函数系列问题与最大似然函数的关系和区别&quot;&gt;2.2  交叉熵函数系列问题？与最大似然函数的关系和区别？&lt;/h4&gt;
&lt;p&gt;在二分类中，交叉熵函数和负最大似然函数的表达式是相同的，但是交叉熵函数是从信息论角度得到的，而最大似然函数则是从概率论角度得到的&lt;/p&gt;

&lt;p&gt;交叉熵涉及到2点：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;信息量：假设X是一个离散型随机变量，其取值集合为X，概率分布函数为p(x)=Pr(X=x),x∈X，我们定义事件X=x0的信息量为：
I(x0)=−log(p(x0))，可以理解为，一个事件发生的概率越大，则它所携带的信息量就越小，而当p(x0)=1时，熵将等于0，也就是说该事件的发生不会导致任何信息量的增加。举个例子，小明平时不爱学习，考试经常不及格，而小王是个勤奋学习的好学生，经常得满分，所以我们可以做如下假设：
事件A：小明考试及格，对应的概率P(xA)=0.1，信息量为I(xA)=−log(0.1)=3.3219
事件B：小王考试及格，对应的概率P(xB)=0.999，信息量为I(xB)=−log(0.999)=0.0014
可以看出，结果非常符合直观：小明及格的可能性很低(十次考试只有一次及格)，因此如果某次考试及格了（大家都会说：XXX竟然及格了！），必然会引入较大的信息量，对应的I值也较高。&lt;/li&gt;
  &lt;li&gt;熵：假设小明的考试结果是一个0-1分布XA只有两个取值{0：不及格，1：及格}，在某次考试结果公布前，小明的考试结果有多大的不确定度呢？你肯定会说：十有八九不及格！因为根据先验知识，小明及格的概率仅有0.1,90%的可能都是不及格的。怎么来度量这个不确定度？求期望！不错，我们对所有可能结果带来的额外信息量求取均值（期望），其结果不就能够衡量出小明考试成绩的不确定度了吗。&lt;strong&gt;熵其实是信息量的期望值，它是一个随机变量的确定性的度量。熵越大，变量的取值越不确定，反之就越确定。&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;相对熵：称为&lt;strong&gt;KL散度&lt;/strong&gt;，是两个随机分布间距离的度量。越小说明分布越一致。&lt;/li&gt;
  &lt;li&gt;交叉熵：交叉熵与KL距离在行为上是等价的，都反映了分布p，q的相似程度。特别的，在logistic regression中，
p:真实样本分布，服从参数为p的0-1分布，即X∼B(1,p)X∼B(1,p)
q:待估计的模型，服从参数为q的0-1分布，即X∼B(1,q)两者的交叉熵为&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;23-svm的核函数&quot;&gt;2.3 SVM的核函数&lt;/h4&gt;
&lt;p&gt;使用非线性核的支持向量机可以处理线性不可分的问题。通过核函数，支持向量机可以将特征向量映射到更高维的空间中，使得原本线性不可分的数据在映射之后的空间中变得线性可分，如下图所示，原本二维空间的线性不可分（异或问题）转成三维空间，就可以线性可分了。
&lt;img src=&quot;https://i.loli.net/2021/02/26/Z1zYvUKGMfy86mb.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;常用的核函数：线性核、多项式核、高斯核（RBF）、拉普拉斯核等等。核函数的选择其实才是SVM模型的最大变数。&lt;/p&gt;

&lt;h3 id=&quot;三-nlp&quot;&gt;三. NLP&lt;/h3&gt;
&lt;h4 id=&quot;31--什么是tf-idf&quot;&gt;3.1  什么是TF-IDF?&lt;/h4&gt;
&lt;p&gt;词频-逆文档频率TF-IDF(term frequency–inverse document frequency)是一种用于信息检索与数据挖掘的常用加权技术，常用于挖掘文章中的关键词，而且算法简单高效，常被工业用于最开始的文本数据清洗。&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\text { 词频(TF) }=\frac{\text { 某个词在文章中的出现次数 }}{\text { 文章的总词数 }}&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\text { 逆文档频率(IDF) }=\log \left(\frac{\text { 语料库的文档总数 }}{\text { 包含该词的文档数 } x+1}\right)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;当有TF(词频)和IDF(逆文档频率)后，将这两个词相乘，就能得到一个词的TF-IDF的值。某个词在文章中的TF-IDF越大，那么一般而言这个词在这篇文章的重要性会越高，所以通过计算文章中各个词的TF-IDF，由大到小排序，排在最前面的几个词，就是该文章的关键词。&lt;/p&gt;

&lt;h4 id=&quot;32--什么是word2vec&quot;&gt;3.2  什么是word2vec&lt;/h4&gt;

&lt;p&gt;判断一个词的词性（动词，名词）这里可以用word2vec，&lt;/p&gt;

&lt;p&gt;嵌入到一个数学空间里，这种嵌入方式，就叫词嵌入（word embedding)，而 Word2vec是词嵌入的一种。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Skip-gram 模型：用一个词语作为输入，来预测它周围的上下文&lt;/li&gt;
  &lt;li&gt;CBOW 模型：拿一个词语的上下文作为输入，来预测这个词语本身&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;33--fasttext&quot;&gt;3.3  fastText&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;word2vec的CBOW模型架构和fastText模型非常相似&lt;/li&gt;
  &lt;li&gt;fastText 和CBOW差别：CBOW的输入是目标单词的上下文，fastText的输入是多个单词及其n-gram特征，这些特征用来表示单个文档；CBOW的输入单词被onehot编码过，fastText的输入特征是被embedding过；CBOW的输出是目标词汇，fastText的输出是文档对应的类标。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;34--ner&quot;&gt;3.4  NER&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;named-entity-recognition（命名实体识别，又叫“专名识别”）。指识别文本中具有特定意义的实体，主要包括人名，地名，机构名，专有名词。&lt;strong&gt;NER系统就是从非结构化的输入文本中抽取出上述实体，并且可以按照业务需求识别出更多类别的实体&lt;/strong&gt;，比如产品名称、型号、价格等。学术上NER所涉及的命名实体一般包括3大类（实体类，时间类，数字类）和7小类（人名、地名、组织机构名、时间、日期、货币、百分比）。货币、百分比等数字类实体可通过正则搞定。&lt;/li&gt;
  &lt;li&gt;NER是NLP中一项基础性关键任务。&lt;strong&gt;从自然语言处理的流程来看，NER可以看作词法分析中未登录词识别的一种，是未登录词中数量最多、识别难度最大、对分词效果影响最大问题。&lt;/strong&gt;同时NER也是关系抽取、事件抽取、知识图谱、机器翻译、问答系统等诸多NLP任务的基础。&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;原句：姚明在NBA打篮球&lt;/p&gt;

  &lt;p&gt;如下标签：姚/B-PER 明/I-PER 在/O NBA/B_ORG 打/O 篮/O 球/O&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;其中常见的方法是对字或者词打上标签。&lt;code class=&quot;highlighter-rouge&quot;&gt;B-type&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;I-type&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;O&lt;/code&gt;， 其中&lt;code class=&quot;highlighter-rouge&quot;&gt;B-type&lt;/code&gt;表示组成该类型实体的第一个字或词。&lt;code class=&quot;highlighter-rouge&quot;&gt;I-type&lt;/code&gt;表示组成该类型实体的中间或最后字或词，&lt;code class=&quot;highlighter-rouge&quot;&gt;O&lt;/code&gt;表示该字或词不组成命名实体，当然有的地方也采用&lt;code class=&quot;highlighter-rouge&quot;&gt;B-type&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;I-type&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;E-type&lt;/code&gt;，&lt;code class=&quot;highlighter-rouge&quot;&gt;O&lt;/code&gt;形式。&lt;/p&gt;

&lt;p&gt;整体结构如下：&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;字（词嵌入）==&amp;gt; BiLSTM（拿到字的每一个标签的所有得分）==&amp;gt; CRF（输出预测标签值）&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
  &lt;li&gt;这里问什么要用到CRF(直接用全连接分类即可)？==&amp;gt; &lt;strong&gt;CRF层能从训练数据中获得约束性的规则&lt;/strong&gt;：CRF层可以为最后预测的标签添加一些约束来保证预测的标签是合法的。在训练数据训练过程中，这些约束可以通过CRF层自动学习到。 这些约束可以是： I：句子中第一个词总是以标签“B-“ 或 “O”开始，而不是“I-” II：标签“B-label1 I-label2 I-label3 I-…”,label1, label2, label3应该属于同一类实体。例如，“B-Person I-Person” 是合法的序列, 但是“B-Person I-Organization” 是非法标签序列. III：标签序列“O I-label” is 非法的.实体标签的首个标签应该是 “B-“ ，而非 “I-“, 换句话说,有效的标签序列应该是“O B-label”。 有了这些约束，标签序列预测中非法序列出现的概率将会大大降低。&lt;/li&gt;
  &lt;li&gt;CRF（条件随机场）：属于判别式模型，条件随机场对多个变量在给定观测值后的条件概率进行建模。概率图模型是以某些可观测的变量为条件分布进行推断。假设某个字的前后（&lt;script type=&quot;math/tex&quot;&gt;x_{1}&lt;/script&gt;,&lt;script type=&quot;math/tex&quot;&gt;x_{2}&lt;/script&gt;,&lt;script type=&quot;math/tex&quot;&gt;x_{3}&lt;/script&gt;）,推断问题的目标就是计算2在1的条件下发生的概率，然后所有条件概率相加。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;35--文本增强技术&quot;&gt;3.5  文本增强技术&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;词汇和短语进行替换&lt;/strong&gt;：选择同义词进行替换；空间中找到相邻的词汇进行替换；利用TF-IDF对哪些非核心词汇（分值很低的）进行替换&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;随机噪音&lt;/strong&gt;：随机插入一些词汇，占位符；交换词汇或者shuffle句子；随机删除词汇或者句子&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;混合增强&lt;/strong&gt;：起源于图像的mixup（猫和狗的混合）。提出了wordMixup和sentMixup将词向量和句向量进行Mixup。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;回译&lt;/strong&gt;：中文翻译成英文表达，然后再由英文翻译回中文。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GAN对抗生成网络&lt;/strong&gt;：GAN 主要分为两部分：生成模型和判别模型。生成模型的作用是模拟真实数据的分布，判别模型的作用是判断一个样本是真实的样本还是生成的样本。GAN 的目标是训练一个生成模型完美的拟合真实数据分布使得判别模型无法区分。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;四-图像算法&quot;&gt;四. 图像算法&lt;/h3&gt;
&lt;blockquote&gt;
  &lt;p&gt;参考文章：
&lt;a href=&quot;https://zhuanlan.zhihu.com/p/80852438&quot;&gt;图像总常用的变换&lt;/a&gt;
&lt;a href=&quot;https://zhuanlan.zhihu.com/p/59640437&quot;&gt;边缘检测&lt;/a&gt;
&lt;a href=&quot;https://blog.csdn.net/samkieth/article/details/49533435&quot;&gt;图像增强技术&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h4 id=&quot;41--图像特征提取的方法有哪些&quot;&gt;4.1  图像特征提取的方法有哪些？&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;SIFT（尺度不变特征变换）—— 图像拼接：&lt;/li&gt;
  &lt;li&gt;HOG（方向梯度直方图（Histogram of Oriented Gradient, HOG））—— 行人检测：特征是一种在计算机视觉和图像处理中用来进行物体检测的特征描述子。它通过计算和统计图像局部区域的梯度方向直方图来构成特征。）&lt;strong&gt;本质：梯度的统计信息，而梯度主要存在于边缘的地方&lt;/strong&gt;。&lt;/li&gt;
  &lt;li&gt;LBP(Local Binary Pattern局部二值模式)：种描述图像局部纹理的特征算子，具有旋转不变性与灰度不变性等显著优点。LBP特征将窗口中心点与邻域点的关系进行比较，重新编码形成新特征以消除对外界场景对图像的影响，因此一定程度上解决了复杂场景下（光照变换）特征描述问题（&lt;strong&gt;局部纹理特征提取&lt;/strong&gt;）。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;42--为什么要图像的灰度化&quot;&gt;4.2  为什么要图像的灰度化?&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;图像识别中要识别物体：找到edge ==&amp;gt; 计算梯度 ==&amp;gt; 需要用到灰度图&lt;/li&gt;
  &lt;li&gt;有利于图像特征提取：RGB采用的是三通道，而灰度图用的是单通道，能加快特征抽取。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;43--为什么预处理中要归一化和标准化&quot;&gt;4.3  为什么预处理中要归一化和标准化&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;取值范围从0～255已经转化为0～1之间了，这个对于后续的神经网络或者卷积神经网络处理有很大的好处，加快梯度下降求解的速度&lt;/li&gt;
  &lt;li&gt;减小了几何变换和仿射变化的影响。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;44--为什么要中值滤波和均值滤波&quot;&gt;4.4  为什么要中值滤波和均值滤波?&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;目的：消除图像中的噪声成分叫作图像的平滑化或滤波操作。图像的能量大部分集中在幅度谱的低频和中频段是很常见的，而在较高频段，感兴趣的信息经常被噪声淹没。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;中值滤波：一连串数字｛1，4，6，8，9｝中，数字6就是这串数字的中值.椒盐噪声很好的被平滑了，而且也没均值那样模糊化太过于严重。&lt;/li&gt;
  &lt;li&gt;均值滤波：图片中一个方块区域（一般为3*3）内，中心点的像素为全部点像素值的平均值。一般均值滤波过于模糊化了。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;45--边缘检测算子&quot;&gt;4.5  边缘检测算子&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Roberts算子：基于x轴和y轴的&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
s_{x}=\left[\begin{array}{cc}
1 &amp; 0 \\
0 &amp; -1
\end{array}\right] %]]&gt;&lt;/script&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
s_{y}=\left[\begin{array}{cc}
0 &amp; -1 \\
1 &amp; 0
\end{array}\right] %]]&gt;&lt;/script&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Prewitt算子：&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
s_{x}=\left[\begin{array}{ccc}
-1 &amp; 0 &amp; 1 \\
-1 &amp; 0 &amp; 1 \\
-1 &amp; 0 &amp; 1
\end{array}\right] %]]&gt;&lt;/script&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
s_{y}=\left[\begin{array}{ccc}
1 &amp; 1 &amp; 1 \\
0 &amp; 0 &amp; 0 \\
-1 &amp; -1 &amp; -1
\end{array}\right] %]]&gt;&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Sobel算子：&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
s_{x}=\left[\begin{array}{ccc}
-1 &amp; 0 &amp; 1 \\
-2 &amp; 0 &amp; 2 \\
-1 &amp; 0 &amp; 1
\end{array}\right] %]]&gt;&lt;/script&gt;&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
s_{y}=\left[\begin{array}{ccc}
1 &amp; 2 &amp; 1 \\
0 &amp; 0 &amp; 0 \\
-1 &amp; -2 &amp; -1
\end{array}\right] %]]&gt;&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;基本的边缘算子&lt;/strong&gt;如Sobel求得的边缘图存在很多问题，如&lt;strong&gt;噪声污染没有被排除、边缘线太过于粗宽&lt;/strong&gt;等&lt;/li&gt;
  &lt;li&gt;Canny算子：目标是找到一个最优的边缘。具有以下优势
    &lt;ul&gt;
      &lt;li&gt;低错误率：标识尽可能多的实际边缘，剑豪噪声产生的误报。&lt;/li&gt;
      &lt;li&gt;高定位性：标识出的边缘要与图像的实际边缘尽可能的接近。&lt;/li&gt;
      &lt;li&gt;最小响应：图像中的边缘只能标识一次。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;canny检测的步骤：
    &lt;ul&gt;
      &lt;li&gt;
        &lt;ol&gt;
          &lt;li&gt;使用高斯滤波器降噪。&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;ol&gt;
          &lt;li&gt;利用Sobel算子进行卷积（x和y反向）&lt;/li&gt;
          &lt;li&gt;将像素点上x和y卷积之后的平方求根，并计算x，y方向上的角度，&lt;script type=&quot;math/tex&quot;&gt;G=\sqrt{G_{x}^{2}+G_{y}^{2}}&lt;/script&gt;，&lt;script type=&quot;math/tex&quot;&gt;\theta=\arctan \left(\frac{G_{y}}{G_{x}}\right)&lt;/script&gt;&lt;/li&gt;
          &lt;li&gt;非极大值抑制，进一步排除非边缘的像素，仅保留一些细线条。&lt;/li&gt;
          &lt;li&gt;滞后阈值：高于某阈值，保留为边缘像素，反之排除。&lt;/li&gt;
        &lt;/ol&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;46--常用的插值方法&quot;&gt;4.6  常用的插值方法&lt;/h4&gt;

&lt;p&gt;在图像几何变换时，无法给有些像素点直接赋值，例如，&lt;strong&gt;将图像放大两倍，必然会多出一些无法被直接映射的像素点，对于这些像素点，通过插值决定它们的值&lt;/strong&gt;。于是，产生了图像插值算法。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;线性插值：&lt;strong&gt;最近邻插值，双线性插值以及双三次插值等&lt;/strong&gt;，&lt;script type=&quot;math/tex&quot;&gt;f(x)=a_{1} x+a_{0}&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;47-深度学习和传统目标检测方法的优缺点&quot;&gt;4.7 深度学习和传统目标检测方法的优缺点&lt;/h4&gt;

&lt;p&gt;传统的目标检测算法对光照，明暗，数据传输，物体遮挡等上模型的鲁棒性不强。&lt;/p&gt;

&lt;h4 id=&quot;48--图像增强技术&quot;&gt;4.8  图像增强技术&lt;/h4&gt;

&lt;p&gt;增强技术也可以有多种分类，如，可以分为平滑（抑制高频成分）与锐化（增强高频成分），空间域与频域。&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;空间域增强就是指增强构成图像的像素，是直接对这些像素进行操作的过程。&lt;/li&gt;
  &lt;li&gt;频域则是修改图像的傅立叶变换。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;49-ssd和yolo&quot;&gt;4.9 SSD和Yolo&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;SSD：将物体检测这个问题的解空间，抽象为一组预先设定好（尺度，长宽比）的bounding box。在每个bounding box，预测分类label，以及box offset来更好的框出物体。对一张图片，结合多个大小不同的feature map的预测结果，以期能够处理大小不同的物体。
    &lt;ul&gt;
      &lt;li&gt;（优点）相比Fast RNN系列，删除了bounding box proposal这一步，及后续的重采样步骤，因而速度较快，达到59FPS。&lt;/li&gt;
      &lt;li&gt;（优点）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;YOLO：&lt;strong&gt;将物体检测这个问题定义为bounding box和分类置信度的回归问题。&lt;/strong&gt;将整张图像作为输入，划分成SxS grid，每个cell预测B个bounding box（x, y, w, h）及对应的分类置信度（class-specific confidence score）。分类置信度是bounding box是物体的概率及其与真实值IOU相乘的结果。
    &lt;ul&gt;
      &lt;li&gt;（优点）速度快，45FPS&lt;/li&gt;
      &lt;li&gt;（优点）YOLO使用图像的全局信息做预测，因而对背景的误识别率低。&lt;/li&gt;
      &lt;li&gt;（缺点） 每个cell只能拥有一个label和两个bounding box，这个空间局限性，使得对小物体检测效果不好&lt;/li&gt;
      &lt;li&gt;（缺点）对于物体长宽比的泛化能力较弱，当一类物体新的长宽比出现时，检测准确率减低。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;二者之间的差别：YOLO在卷积层后接全连接层，即检测时只利用了最高层Feature maps（包括Faster RCNN也是如此）而SSD采用金字塔结构，即利用了conv4-3/fc7/conv6-2/conv7-2/conv8_2/conv9_2这些大小不同的feature maps，在多个feature maps上同时进行softmax分类和位置回归。SSD还加入了Prior box&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;410--零样本学习zero-shot-learning和单样本学习one-shot-learning&quot;&gt;4.10  零样本学习（Zero-shot Learning）和单样本学习（One-shot Learning）&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;零样本学习：基于可见标注数据集&amp;amp;可见标签集合（seen），学习并预测不可见（unseen，无标注）数据集结果。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;411--前景背景分割&quot;&gt;4.11  前景背景分割&lt;/h4&gt;

&lt;h4 id=&quot;412--工业相机ccd和cmos&quot;&gt;4.12  工业相机CCD和CMOS&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;CCD（电荷耦合元件）：输出节点统一输出数据，信号一致性好；CCD采用逐个光敏输出，速度较慢&lt;/li&gt;
  &lt;li&gt;CMOS（金属氧化物半导体元件）：CMOS芯片中每个像素都有自己的信号放大器，各自进行电荷到电压的转换，输出信号的一致性较差，比CCD的信号噪声更多。CMOS每个电荷元件都有独立的装换控制器，读出速度很快，FPS在500以上的高速相机大部分使用的都是CMOS。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;413-小目标检测&quot;&gt;4.13 小目标检测&lt;/h4&gt;
&lt;p&gt;在深度学习目标检测中，特别是人脸检测中，小目标、小人脸的检测由于&lt;strong&gt;分辨率低，图片模糊，信息少，噪音多&lt;/strong&gt;，所以一直是一个实际且常见的困难问题。
FPN特征金字塔网络：参考文章：https://zhuanlan.zhihu.com/p/92005927&lt;/p&gt;

&lt;h4 id=&quot;414-目标检测中的map&quot;&gt;4.14 目标检测中的mAP&lt;/h4&gt;
&lt;p&gt;具体参考文章：https://www.cnblogs.com/itmorn/p/14193729.html&lt;/p&gt;

&lt;h3 id=&quot;五-深度学习&quot;&gt;五. 深度学习&lt;/h3&gt;
&lt;h4 id=&quot;51--梯度消失的原因和解决办法有哪些&quot;&gt;5.1  梯度消失的原因和解决办法有哪些？&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;梯度消失：每一层非线性层都可以视为是一个非线性函数 f(x)f(x)(非线性来自于非线性激活函数），因此整个深度网络可以视为是一个复合的非线性多元函数。那么根据“链式求导”法则，比如rnn来说，其激活函数为tanh，那么tanh的导数的最大值是1，那么如果连乘0.8的100次方，无线接近于0，导致梯度消失。&lt;/li&gt;
  &lt;li&gt;梯度爆炸：tanh导数 * W权重，这里如果W的值太大了，随着序列长度的增加，连乘无限大，导致梯度爆炸。&lt;/li&gt;
  &lt;li&gt;解决方案：一个是激活函数比如relu系列，一个初始化权重 ，一个是梯度裁剪&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;52--rnn-和lstm的差别在哪&quot;&gt;5.2  RNN 和LSTM的差别在哪？&lt;/h4&gt;

&lt;p&gt;RNN的前向推导公式：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://tva1.sinaimg.cn/large/007S8ZIlgy1giw0543q2gj30ep07bq34.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;LSTM的三种门控制如下：
&lt;img src=&quot;https://tva1.sinaimg.cn/large/007S8ZIlgy1giw15g44kej311m0migvc.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;如上图所示，它们的名字、表示的计算过程及输出分别是：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://tva1.sinaimg.cn/large/007S8ZIlgy1giw16ne833j30is04ygm9.jpg&quot; alt=&quot;&quot; /&gt;
可以看到，除了参数不同，它们计算公式是一样的。啰嗦一句，上图中 [公式] 表示sigmoid函数， [公式] 表示tanh函数：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;RNN来说，它能够处理一定的短期依赖，但无法处理长期依赖问题。原因：当序列较长时，序列后部的梯度很难反向传播到前面的序列，比如10个元素以前，这就产生了梯度消失问题&lt;/li&gt;
  &lt;li&gt;当然，RNN也存在梯度爆炸问题，但这个问题一般可以通过梯度裁剪（gradient clipping）来解决&lt;/li&gt;
  &lt;li&gt;RNN没有细胞状态；LSTM通过细胞状态记忆信息。&lt;/li&gt;
  &lt;li&gt;RNN激活函数只有tanh；LSTM通过输入门、遗忘门、输出门引入sigmoid函数并结合tanh函数，添加求和操作，减少梯度消失和梯度爆炸的可能性。&lt;/li&gt;
  &lt;li&gt;RNN只能够处理短期依赖问题；LSTM既能够处理短期依赖问题，又能够处理长期依赖问题。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;53--注意力机制是为了解决什么问题为什么选用了双向循环神经网络&quot;&gt;5.3  注意力机制是为了解决什么问题？为什么选用了双向循环神经网络？&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;人脑在工作时具有一定注意力，当欣赏艺术品时，既可以看到全貌，也可以关注 细节，眼睛聚焦在局部，忽略其他位置信息。说明人脑在处理信息的时候有一定权重划分。而注意力机制的提出正是模仿了人脑的这种核心特性。&lt;/li&gt;
  &lt;li&gt;实际使用中，随着输入序列长度的增加，模型性能显著下降。因为&lt;strong&gt;编码时输入序列的全部信息被压缩到一个向量表示中去&lt;/strong&gt;。&lt;strong&gt;序列越长，句子越前面的词的信息丢失就越严重&lt;/strong&gt;。以100词的句子为例，编码时将整个句子的信息压缩到一个向量中去，而在解码时(比如翻译)，目标语言第一个单词大概率与源语言第一个单词对应，这就意味着第一步的解码需要考虑到100步之前的信息。一个小技巧是可以将&lt;strong&gt;源语言句子逆向输入&lt;/strong&gt;，或者重复输入两遍，得到一定的提升，也可以使用&lt;strong&gt;LSTM&lt;/strong&gt;缓解这个问题。但对于&lt;strong&gt;过长序列仍难以有很好表现&lt;/strong&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;54--batch-normalization和dropout差别&quot;&gt;5.4  Batch Normalization和Dropout差别&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;BN训练和测试时的参数是一样的嘛？&lt;/strong&gt;BN是对每一批训练数据进行归一化，使用每一批数据的均值和方差；测试的时候，每一批数据中仅有一个样本，没有batch概念了，这里的均值和方差就是全量数据均值和方差。&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;BN训练时为什么不用全量训练集的均值和方差呢？&lt;/strong&gt;对于BN，是对每一批数据进行归一化到一个相同的分布，而每一批数据的均值和方差会有一定的差别，而不是用固定的值，这个差别实际上也能够增加模型的鲁棒性，也会在一定程度上减少过拟合。&lt;strong&gt;BN操作把分布压缩在[-1,1],服从均值为0,方差为1的正太分布&lt;/strong&gt;，相当于把大部分Activation的值落入非线性函数的线性区内，其对应的导数远离导数饱和区，这样来加速训练收敛过程。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dropout的作用是什么？&lt;/strong&gt; 在训练的过程中以一定概率使得神经元失活，即输出为0，以提高模型的泛化能力，减少过拟合。&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dropout 在训练和测试时都需要嘛？&lt;/strong&gt;dropout仅在训练的时候采用，为了减少神经元对部分上层神经元的依赖，类似于将多个不同的网络结构的模型集成起来，减少过拟合和增强其鲁棒性。测试的时候用到的是整个训练完成的模型，不需要dropout。&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Dropout 如何平衡训练和测试时的差异呢？&lt;/strong&gt;假设失活概率为 p ，就是这一层中的每个神经元都有p的概率失活，如下图的三层网络结构中，如果失活概率为0.5，则平均每一次训练有3个神经元失活，所以输出层每个神经元只有3个输入，而实际测试时是不会有dropout的，输出层每个神经元都有6个输入，这样在训练和测试时，输出层每个神经元的输入和的期望会有量级上的差异。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;BN和Dropout共同使用时会出现的问题&lt;/strong&gt;BN和Dropout单独使用都能减少过拟合并加速训练速度，但如果一起使用的话并不会产生1+1&amp;gt;2的效果，相反可能会得到比单独使用更差的效果。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;55--batch-normalization和layer-normalization的差别&quot;&gt;5.5  Batch Normalization和Layer Normalization的差别&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;LN和BN都是一种归一化方式，差别是：BN是取的是不同样本的同一个特征进行归一化；LN取得是同一个样本的不同特征。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;应用场景不同：LN适用于RNN或者batchsize较小；BN适用于CNN。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;对于RNN来说，每个样本的长度都是不同的，那么当BN需要统计靠后的时间片段的时候，可能都没有这方面的信息，那么只基于某些长时间片段的样本的统计信息无法反应出全局分布，所以就不合适了。&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;56--bert的具体网络结构以及训练过程及其优势在哪&quot;&gt;5.6  bert的具体网络结构，以及训练过程，及其优势在哪&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;bert处理句子是整体处理的，不是逐字处理的，解决了不受长期依赖问题困扰的主要原因（不存在过去信息丢失的风险），同时提高了训练效率。&lt;/li&gt;
  &lt;li&gt;多头注意力和位置嵌入：提供了有关不同单词之间的关系信息。&lt;/li&gt;
  &lt;li&gt;总结：完全避免了递归操作，通过整体处理句子以及学习单词之间的关系来感谢多头注意机制和位置嵌入。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;57--albert和bert的差别在哪&quot;&gt;5.7  albert和bert的差别在哪&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;albert的核心：&lt;strong&gt;训练出更小但效果更好的模型!&lt;/strong&gt; 想让模型更轻，训练更快，效果更好！（期望的是&lt;strong&gt;用更少量的数据，得到更好的结果&lt;/strong&gt;）。ALBERT提出了三种优化策略，做到了比BERT模型小很多的模型，但效果反而超越了BERT， XLNet。
    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Factorized Embedding Parameterization&lt;/strong&gt;. 他们做的第一个改进是针对于Vocabulary Embedding。在BERT、XLNet中，词表的embedding size(E)和transformer层的hidden size(H)是等同的，所以E=H。但实际上词库的大小一般都很大，这就导致模型参数个数就会变得很大。为了解决这些问题他们提出了一个基于factorization的方法。他们没有直接把one-hot映射到hidden layer, 而是先把one-hot映射到低维空间之后，再映射到hidden layer。这其实类似于做了矩阵的分解。&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Cross-layer parameter sharing&lt;/strong&gt;. 每一层的layer可以共享参数，这样一来参数的个数不会以层数的增加而增加。所以最后得出来的模型相比BERT-large小18倍以上。&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Inter-sentence coherence loss&lt;/strong&gt;. 在BERT的训练中提出了next sentence prediction loss, 也就是给定两个sentence segments, 然后让BERT去预测它俩之间的先后顺序，但在ALBERT文章里提出这种是有问题的，其实也说明这种训练方式用处不是很大。 所以他们做出了改进，他们使用的是setence-order prediction loss (SOP)，其实是基于主题的关联去预测是否两个句子调换了顺序。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;58--cnn和rnn的差别&quot;&gt;5.8  CNN和RNN的差别&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;训练速度上：CNN快很多。RNN慢的原因是每个timestep的计算，都要依赖前一个时刻的输出。而cnn的卷积的时候，和空间上其他的点没有任何联系，适合并行计算。&lt;/li&gt;
  &lt;li&gt;数据约束：CNN对于数据的约束就很强了，图像识别，input的纬度是48*48的，必须定死了，而RNN其实对于数据的长度（句子的长度）没有要求（TF里面有动态rnn来在输入rnn之前去掉pad为0的地方）&lt;/li&gt;
  &lt;li&gt;卷积层不同空间位置的神经元共享权值，用于发现图像中不同空间位置的模式。共享参数是深度学习一个重要的思想，其在减少网络参数的同时仍然能保持很高的网络容量(capacity)。卷积层在空间方向共享参数，而循环神经网络(recurrent neural networks)在时间方向共享参数。&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;59--深度学习平台&quot;&gt;5.9  深度学习平台&lt;/h4&gt;
&lt;ul&gt;
  &lt;li&gt;阿里NASA计划的机器学习平台PAI（17年）
    &lt;ul&gt;
      &lt;li&gt;全面兼容TF，Caffe，MXNet深度学习框架&lt;/li&gt;
      &lt;li&gt;提供云端的计算资源&lt;/li&gt;
      &lt;li&gt;集成很多机器学习算法（分类，回归，聚类）&lt;/li&gt;
      &lt;li&gt;支持大规模的分布式数据训练&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;百度paddlepaddle飞桨(18年)
    &lt;ul&gt;
      &lt;li&gt;支持大规模的分布式数据训练&lt;/li&gt;
      &lt;li&gt;多平台部署&lt;/li&gt;
      &lt;li&gt;产业级的开源模型库（语义理解，图像分类，目标检测，图像分割等多种场景）&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;微软Microsoft Custom Vision Services（17年）
    &lt;ul&gt;
      &lt;li&gt;针对的是图像分类器&lt;/li&gt;
      &lt;li&gt;提供迁移学习的模型&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;谷歌的Cloud AutoML
    &lt;ul&gt;
      &lt;li&gt;针对的是图像分类器&lt;/li&gt;
      &lt;li&gt;提供迁移学习的模型&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name></name>
        
        
      </author>

      

      
        <category term="DeepLearning" />
      
        <category term="MachineLearning" />
      

      
        <summary type="html">面试中机器学习中常问的问题总结</summary>
      

      
      
    </entry>
  
</feed>

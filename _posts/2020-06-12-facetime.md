---
layout: post
current: post
cover: assets/images/question.jpg
navigation: True
title: 面试总结
date: 2020-06-12  15:21:00
tags: [Machinelearning, 深度学习, CV, NLP]
class: post-template
subclass: 'post'
---



> 面试官会根据自己简历中提到的一些点进行提问，这里先自己对某些点进行深挖。

### 一.数据处理
海量数据：
* （1）数据量太大，无法短时间内处理完成 
* （2）无法一次性将数据放入内存中。

#### 1.1 缺失值处理
* 填充固定值：选取某个固定值/默认值填充缺失值。
* 填充均值：对每一列的缺失值，填充当列的均值。
* 填充中位数：对每一列的缺失值，填充当列的中位数。
* 填充众数：对每一列的缺失值，填充当列的众数。由于存在某列缺失值过多，众数为nan的情况，因此这里取的是每列删除掉nan值后的众数。
* 填充上下条的数据：对每一条数据的缺失值，填充其上下条数据的值。
* 填充插值得到的数据：用插值法拟合出缺失的数据，然后进行填充。插值是离散函数逼近的重要方法，利用它可通过函数在有限个点处的取值状况，估算出函数在其他点处的近似值。


### 二.机器学习
#### 2.1  SVM和LR的区别与联系？
SVM 和 LR 都是属于分类算法，不过 SVM 是通过划分超平面的方法来进行分类，而 LR 则是通过计算样本属于哪个类别的概率，从而达到分类效果

#### 2.2  交叉熵函数系列问题？与最大似然函数的关系和区别？
在二分类中，交叉熵函数和负最大似然函数的表达式是相同的，但是交叉熵函数是从信息论角度得到的，而最大似然函数则是从概率论角度得到的

交叉熵涉及到2点：
* 信息量：假设X是一个离散型随机变量，其取值集合为X，概率分布函数为p(x)=Pr(X=x),x∈X，我们定义事件X=x0的信息量为：
I(x0)=−log(p(x0))，可以理解为，一个事件发生的概率越大，则它所携带的信息量就越小，而当p(x0)=1时，熵将等于0，也就是说该事件的发生不会导致任何信息量的增加。举个例子，小明平时不爱学习，考试经常不及格，而小王是个勤奋学习的好学生，经常得满分，所以我们可以做如下假设：
事件A：小明考试及格，对应的概率P(xA)=0.1，信息量为I(xA)=−log(0.1)=3.3219
事件B：小王考试及格，对应的概率P(xB)=0.999，信息量为I(xB)=−log(0.999)=0.0014
可以看出，结果非常符合直观：小明及格的可能性很低(十次考试只有一次及格)，因此如果某次考试及格了（大家都会说：XXX竟然及格了！），必然会引入较大的信息量，对应的I值也较高。
* 熵：假设小明的考试结果是一个0-1分布XA只有两个取值{0：不及格，1：及格}，在某次考试结果公布前，小明的考试结果有多大的不确定度呢？你肯定会说：十有八九不及格！因为根据先验知识，小明及格的概率仅有0.1,90%的可能都是不及格的。怎么来度量这个不确定度？求期望！不错，我们对所有可能结果带来的额外信息量求取均值（期望），其结果不就能够衡量出小明考试成绩的不确定度了吗。**熵其实是信息量的期望值，它是一个随机变量的确定性的度量。熵越大，变量的取值越不确定，反之就越确定。**
* 相对熵：称为**KL散度**，是两个随机分布间距离的度量。越小说明分布越一致。
* 交叉熵：交叉熵与KL距离在行为上是等价的，都反映了分布p，q的相似程度。特别的，在logistic regression中，
  p:真实样本分布，服从参数为p的0-1分布，即X∼B(1,p)X∼B(1,p)
  q:待估计的模型，服从参数为q的0-1分布，即X∼B(1,q)两者的交叉熵为


### 三. NLP
#### 3.1  什么是TF-IDF?
词频-逆文档频率TF-IDF(term frequency–inverse document frequency)是一种用于信息检索与数据挖掘的常用加权技术，常用于挖掘文章中的关键词，而且算法简单高效，常被工业用于最开始的文本数据清洗。

$$\text { 词频(TF) }=\frac{\text { 某个词在文章中的出现次数 }}{\text { 文章的总词数 }}$$
$$\text { 逆文档频率(IDF) }=\log \left(\frac{\text { 语料库的文档总数 }}{\text { 包含该词的文档数 } x+1}\right)$$

当有TF(词频)和IDF(逆文档频率)后，将这两个词相乘，就能得到一个词的TF-IDF的值。某个词在文章中的TF-IDF越大，那么一般而言这个词在这篇文章的重要性会越高，所以通过计算文章中各个词的TF-IDF，由大到小排序，排在最前面的几个词，就是该文章的关键词。

#### 3.2  什么是word2vec

判断一个词的词性（动词，名词）这里可以用word2vec，

嵌入到一个数学空间里，这种嵌入方式，就叫词嵌入（word embedding)，而 Word2vec是词嵌入的一种。

* Skip-gram 模型：用一个词语作为输入，来预测它周围的上下文
* CBOW 模型：拿一个词语的上下文作为输入，来预测这个词语本身

#### 3.3  fastText
* word2vec的CBOW模型架构和fastText模型非常相似
* fastText 和CBOW差别：CBOW的输入是目标单词的上下文，fastText的输入是多个单词及其n-gram特征，这些特征用来表示单个文档；CBOW的输入单词被onehot编码过，fastText的输入特征是被embedding过；CBOW的输出是目标词汇，fastText的输出是文档对应的类标。

#### 3.4  NER
* named-entity-recognition（命名实体识别，又叫“专名识别”）。指识别文本中具有特定意义的实体，主要包括人名，地名，机构名，专有名词。**NER系统就是从非结构化的输入文本中抽取出上述实体，并且可以按照业务需求识别出更多类别的实体**，比如产品名称、型号、价格等。学术上NER所涉及的命名实体一般包括3大类（实体类，时间类，数字类）和7小类（人名、地名、组织机构名、时间、日期、货币、百分比）。货币、百分比等数字类实体可通过正则搞定。
* NER是NLP中一项基础性关键任务。**从自然语言处理的流程来看，NER可以看作词法分析中未登录词识别的一种，是未登录词中数量最多、识别难度最大、对分词效果影响最大问题。**同时NER也是关系抽取、事件抽取、知识图谱、机器翻译、问答系统等诸多NLP任务的基础。

> 原句：姚明在NBA打篮球
>
> 如下标签：姚/B-PER 明/I-PER 在/O NBA/B_ORG 打/O 篮/O 球/O

其中常见的方法是对字或者词打上标签。`B-type`, `I-type`, `O`， 其中`B-type`表示组成该类型实体的第一个字或词。`I-type`表示组成该类型实体的中间或最后字或词，`O`表示该字或词不组成命名实体，当然有的地方也采用`B-type`, `I-type`, `E-type`，`O`形式。

整体结构如下：

> 字（词嵌入）==> BiLSTM（拿到字的每一个标签的所有得分）==> CRF（输出预测标签值）

*  这里问什么要用到CRF(直接用全连接分类即可)？==> **CRF层能从训练数据中获得约束性的规则**：CRF层可以为最后预测的标签添加一些约束来保证预测的标签是合法的。在训练数据训练过程中，这些约束可以通过CRF层自动学习到。 这些约束可以是： I：句子中第一个词总是以标签“B-“ 或 “O”开始，而不是“I-” II：标签“B-label1 I-label2 I-label3 I-…”,label1, label2, label3应该属于同一类实体。例如，“B-Person I-Person” 是合法的序列, 但是“B-Person I-Organization” 是非法标签序列. III：标签序列“O I-label” is 非法的.实体标签的首个标签应该是 “B-“ ，而非 “I-“, 换句话说,有效的标签序列应该是“O B-label”。 有了这些约束，标签序列预测中非法序列出现的概率将会大大降低。
* CRF（条件随机场）：属于判别式模型，条件随机场对多个变量在给定观测值后的条件概率进行建模。概率图模型是以某些可观测的变量为条件分布进行推断。假设某个字的前后（$$x_{1}$$,$$x_{2}$$,$$x_{3}$$）,推断问题的目标就是计算2在1的条件下发生的概率，然后所有条件概率相加。

#### 3.5  文本增强技术

* **词汇和短语进行替换**：选择同义词进行替换；空间中找到相邻的词汇进行替换；利用TF-IDF对哪些非核心词汇（分值很低的）进行替换
* **随机噪音**：随机插入一些词汇，占位符；交换词汇或者shuffle句子；随机删除词汇或者句子
* **混合增强**：起源于图像的mixup（猫和狗的混合）。提出了wordMixup和sentMixup将词向量和句向量进行Mixup。
* **回译**：中文翻译成英文表达，然后再由英文翻译回中文。
* **GAN对抗生成网络**：GAN 主要分为两部分：生成模型和判别模型。生成模型的作用是模拟真实数据的分布，判别模型的作用是判断一个样本是真实的样本还是生成的样本。GAN 的目标是训练一个生成模型完美的拟合真实数据分布使得判别模型无法区分。

### 四. 图像算法
> 参考文章：
> [图像总常用的变换](https://zhuanlan.zhihu.com/p/80852438)
> [边缘检测](https://zhuanlan.zhihu.com/p/59640437)
> [图像增强技术](https://blog.csdn.net/samkieth/article/details/49533435)


#### 4.1  图像特征提取的方法有哪些？

* SIFT（尺度不变特征变换）—— 图像拼接：
* HOG（方向梯度直方图（Histogram of Oriented Gradient, HOG））—— 行人检测：特征是一种在计算机视觉和图像处理中用来进行物体检测的特征描述子。它通过计算和统计图像局部区域的梯度方向直方图来构成特征。）**本质：梯度的统计信息，而梯度主要存在于边缘的地方**。
* LBP(Local Binary Pattern局部二值模式)：种描述图像局部纹理的特征算子，具有旋转不变性与灰度不变性等显著优点。LBP特征将窗口中心点与邻域点的关系进行比较，重新编码形成新特征以消除对外界场景对图像的影响，因此一定程度上解决了复杂场景下（光照变换）特征描述问题（**局部纹理特征提取**）。

#### 4.2  为什么要图像的灰度化?

* 图像识别中要识别物体：找到edge ==> 计算梯度 ==> 需要用到灰度图
* 有利于图像特征提取：RGB采用的是三通道，而灰度图用的是单通道，能加快特征抽取。

#### 4.3  为什么预处理中要归一化和标准化

* 取值范围从0～255已经转化为0～1之间了，这个对于后续的神经网络或者卷积神经网络处理有很大的好处，加快梯度下降求解的速度
* 减小了几何变换和仿射变化的影响。

#### 4.4  为什么要中值滤波和均值滤波?

* 目的：消除图像中的噪声成分叫作图像的平滑化或滤波操作。图像的能量大部分集中在幅度谱的低频和中频段是很常见的，而在较高频段，感兴趣的信息经常被噪声淹没。

* 中值滤波：一连串数字｛1，4，6，8，9｝中，数字6就是这串数字的中值.椒盐噪声很好的被平滑了，而且也没均值那样模糊化太过于严重。
* 均值滤波：图片中一个方块区域（一般为3*3）内，中心点的像素为全部点像素值的平均值。一般均值滤波过于模糊化了。

#### 4.5  边缘检测算子

* Roberts算子：基于x轴和y轴的$$s_{x}=\left[\begin{array}{cc}
  1 & 0 \\
  0 & -1
  \end{array}\right]$$$$s_{y}=\left[\begin{array}{cc}
  0 & -1 \\
  1 & 0
  \end{array}\right]$$

* Prewitt算子：$$s_{x}=\left[\begin{array}{ccc}
  -1 & 0 & 1 \\
  -1 & 0 & 1 \\
  -1 & 0 & 1
  \end{array}\right]$$$$s_{y}=\left[\begin{array}{ccc}
  1 & 1 & 1 \\
  0 & 0 & 0 \\
  -1 & -1 & -1
  \end{array}\right]$$
* Sobel算子：$$s_{x}=\left[\begin{array}{ccc}
  -1 & 0 & 1 \\
  -2 & 0 & 2 \\
  -1 & 0 & 1
  \end{array}\right]$$$$s_{y}=\left[\begin{array}{ccc}
  1 & 2 & 1 \\
  0 & 0 & 0 \\
  -1 & -2 & -1
  \end{array}\right]$$
* **基本的边缘算子**如Sobel求得的边缘图存在很多问题，如**噪声污染没有被排除、边缘线太过于粗宽**等
* Canny算子：目标是找到一个最优的边缘。具有以下优势
  * 低错误率：标识尽可能多的实际边缘，剑豪噪声产生的误报。
  * 高定位性：标识出的边缘要与图像的实际边缘尽可能的接近。
  * 最小响应：图像中的边缘只能标识一次。
* canny检测的步骤：
  * 1. 使用高斯滤波器降噪。
  * 2. 利用Sobel算子进行卷积（x和y反向）
    3. 将像素点上x和y卷积之后的平方求根，并计算x，y方向上的角度，$$G=\sqrt{G_{x}^{2}+G_{y}^{2}}$$，$$\theta=\arctan \left(\frac{G_{y}}{G_{x}}\right)$$
    4. 非极大值抑制，进一步排除非边缘的像素，仅保留一些细线条。
    5. 滞后阈值：高于某阈值，保留为边缘像素，反之排除。

#### 4.6  常用的插值方法

在图像几何变换时，无法给有些像素点直接赋值，例如，**将图像放大两倍，必然会多出一些无法被直接映射的像素点，对于这些像素点，通过插值决定它们的值**。于是，产生了图像插值算法。

* 线性插值：**最近邻插值，双线性插值以及双三次插值等**，$$f(x)=a_{1} x+a_{0}$$

#### 4.7 深度学习和传统目标检测方法的优缺点

传统的目标检测算法对光照，明暗，数据传输，物体遮挡等上模型的鲁棒性不强。

#### 4.8  图像增强技术

增强技术也可以有多种分类，如，可以分为平滑（抑制高频成分）与锐化（增强高频成分），空间域与频域。

* 空间域增强就是指增强构成图像的像素，是直接对这些像素进行操作的过程。
* 频域则是修改图像的傅立叶变换。

#### 4.9 SSD和Yolo

* SSD：将物体检测这个问题的解空间，抽象为一组预先设定好（尺度，长宽比）的bounding box。在每个bounding box，预测分类label，以及box offset来更好的框出物体。对一张图片，结合多个大小不同的feature map的预测结果，以期能够处理大小不同的物体。
  * （优点）相比Fast RNN系列，删除了bounding box proposal这一步，及后续的重采样步骤，因而速度较快，达到59FPS。
  * （优点）
* YOLO：**将物体检测这个问题定义为bounding box和分类置信度的回归问题。**将整张图像作为输入，划分成SxS grid，每个cell预测B个bounding box（x, y, w, h）及对应的分类置信度（class-specific confidence score）。分类置信度是bounding box是物体的概率及其与真实值IOU相乘的结果。
  * （优点）速度快，45FPS
  * （优点）YOLO使用图像的全局信息做预测，因而对背景的误识别率低。
  * （缺点） 每个cell只能拥有一个label和两个bounding box，这个空间局限性，使得对小物体检测效果不好
  * （缺点）对于物体长宽比的泛化能力较弱，当一类物体新的长宽比出现时，检测准确率减低。
* 二者之间的差别：YOLO在卷积层后接全连接层，即检测时只利用了最高层Feature maps（包括Faster RCNN也是如此）而SSD采用金字塔结构，即利用了conv4-3/fc7/conv6-2/conv7-2/conv8_2/conv9_2这些大小不同的feature maps，在多个feature maps上同时进行softmax分类和位置回归。SSD还加入了Prior box

#### 4.10  零样本学习（Zero-shot Learning）和单样本学习（One-shot Learning）

* 零样本学习：基于可见标注数据集&可见标签集合（seen），学习并预测不可见（unseen，无标注）数据集结果。

#### 4.11  前景背景分割

#### 4.12  工业相机CCD和CMOS
* CCD（电荷耦合元件）：输出节点统一输出数据，信号一致性好；CCD采用逐个光敏输出，速度较慢
* CMOS（金属氧化物半导体元件）：CMOS芯片中每个像素都有自己的信号放大器，各自进行电荷到电压的转换，输出信号的一致性较差，比CCD的信号噪声更多。CMOS每个电荷元件都有独立的装换控制器，读出速度很快，FPS在500以上的高速相机大部分使用的都是CMOS。


### 五. 深度学习
#### 5.1  梯度消失的原因和解决办法有哪些？
* 梯度消失：每一层非线性层都可以视为是一个非线性函数 f(x)f(x)(非线性来自于非线性激活函数），因此整个深度网络可以视为是一个复合的非线性多元函数。那么根据“链式求导”法则，比如rnn来说，其激活函数为tanh，那么tanh的导数的最大值是1，那么如果连乘0.8的100次方，无线接近于0，导致梯度消失。
* 梯度爆炸：tanh导数 * W权重，这里如果W的值太大了，随着序列长度的增加，连乘无限大，导致梯度爆炸。
* 解决方案：一个是激活函数比如relu系列，一个初始化权重 ，一个是梯度裁剪

#### 5.2  RNN 和LSTM的差别在哪？

RNN的前向推导公式：

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1giw0543q2gj30ep07bq34.jpg)

LSTM的三种门控制如下：
![](https://tva1.sinaimg.cn/large/007S8ZIlgy1giw15g44kej311m0migvc.jpg)

如上图所示，它们的名字、表示的计算过程及输出分别是：

![](https://tva1.sinaimg.cn/large/007S8ZIlgy1giw16ne833j30is04ygm9.jpg)
可以看到，除了参数不同，它们计算公式是一样的。啰嗦一句，上图中 [公式] 表示sigmoid函数， [公式] 表示tanh函数：


* RNN来说，它能够处理一定的短期依赖，但无法处理长期依赖问题。原因：当序列较长时，序列后部的梯度很难反向传播到前面的序列，比如10个元素以前，这就产生了梯度消失问题
* 当然，RNN也存在梯度爆炸问题，但这个问题一般可以通过梯度裁剪（gradient clipping）来解决
* RNN没有细胞状态；LSTM通过细胞状态记忆信息。
* RNN激活函数只有tanh；LSTM通过输入门、遗忘门、输出门引入sigmoid函数并结合tanh函数，添加求和操作，减少梯度消失和梯度爆炸的可能性。
* RNN只能够处理短期依赖问题；LSTM既能够处理短期依赖问题，又能够处理长期依赖问题。

#### 5.3  注意力机制是为了解决什么问题？为什么选用了双向循环神经网络？
* 人脑在工作时具有一定注意力，当欣赏艺术品时，既可以看到全貌，也可以关注 细节，眼睛聚焦在局部，忽略其他位置信息。说明人脑在处理信息的时候有一定权重划分。而注意力机制的提出正是模仿了人脑的这种核心特性。
* 实际使用中，随着输入序列长度的增加，模型性能显著下降。因为**编码时输入序列的全部信息被压缩到一个向量表示中去**。**序列越长，句子越前面的词的信息丢失就越严重**。以100词的句子为例，编码时将整个句子的信息压缩到一个向量中去，而在解码时(比如翻译)，目标语言第一个单词大概率与源语言第一个单词对应，这就意味着第一步的解码需要考虑到100步之前的信息。一个小技巧是可以将**源语言句子逆向输入**，或者重复输入两遍，得到一定的提升，也可以使用**LSTM**缓解这个问题。但对于**过长序列仍难以有很好表现**。

#### 5.4  Batch Normalization和Dropout差别
* **BN训练和测试时的参数是一样的嘛？**BN是对每一批训练数据进行归一化，使用每一批数据的均值和方差；测试的时候，每一批数据中仅有一个样本，没有batch概念了，这里的均值和方差就是全量数据均值和方差。
* **BN训练时为什么不用全量训练集的均值和方差呢？**对于BN，是对每一批数据进行归一化到一个相同的分布，而每一批数据的均值和方差会有一定的差别，而不是用固定的值，这个差别实际上也能够增加模型的鲁棒性，也会在一定程度上减少过拟合。**BN操作把分布压缩在[-1,1],服从均值为0,方差为1的正太分布**，相当于把大部分Activation的值落入非线性函数的线性区内，其对应的导数远离导数饱和区，这样来加速训练收敛过程。

* **Dropout的作用是什么？** 在训练的过程中以一定概率使得神经元失活，即输出为0，以提高模型的泛化能力，减少过拟合。
* **Dropout 在训练和测试时都需要嘛？**dropout仅在训练的时候采用，为了减少神经元对部分上层神经元的依赖，类似于将多个不同的网络结构的模型集成起来，减少过拟合和增强其鲁棒性。测试的时候用到的是整个训练完成的模型，不需要dropout。
* **Dropout 如何平衡训练和测试时的差异呢？**假设失活概率为 p ，就是这一层中的每个神经元都有p的概率失活，如下图的三层网络结构中，如果失活概率为0.5，则平均每一次训练有3个神经元失活，所以输出层每个神经元只有3个输入，而实际测试时是不会有dropout的，输出层每个神经元都有6个输入，这样在训练和测试时，输出层每个神经元的输入和的期望会有量级上的差异。

*  **BN和Dropout共同使用时会出现的问题**BN和Dropout单独使用都能减少过拟合并加速训练速度，但如果一起使用的话并不会产生1+1>2的效果，相反可能会得到比单独使用更差的效果。

#### 5.5  Batch Normalization和Layer Normalization的差别
* LN和BN都是一种归一化方式，差别是：BN是取的是不同样本的同一个特征进行归一化；LN取得是同一个样本的不同特征。

* 应用场景不同：LN适用于RNN或者batchsize较小；BN适用于CNN。

* 对于RNN来说，每个样本的长度都是不同的，那么当BN需要统计靠后的时间片段的时候，可能都没有这方面的信息，那么只基于某些长时间片段的样本的统计信息无法反应出全局分布，所以就不合适了。

#### 5.6  bert的具体网络结构，以及训练过程，及其优势在哪
* bert处理句子是整体处理的，不是逐字处理的，解决了不受长期依赖问题困扰的主要原因（不存在过去信息丢失的风险），同时提高了训练效率。
* 多头注意力和位置嵌入：提供了有关不同单词之间的关系信息。
* 总结：完全避免了递归操作，通过整体处理句子以及学习单词之间的关系来感谢多头注意机制和位置嵌入。

#### 5.7  albert和bert的差别在哪
* albert的核心：**训练出更小但效果更好的模型!** 想让模型更轻，训练更快，效果更好！（期望的是**用更少量的数据，得到更好的结果**）。ALBERT提出了三种优化策略，做到了比BERT模型小很多的模型，但效果反而超越了BERT， XLNet。
  * **Factorized Embedding Parameterization**. 他们做的第一个改进是针对于Vocabulary Embedding。在BERT、XLNet中，词表的embedding size(E)和transformer层的hidden size(H)是等同的，所以E=H。但实际上词库的大小一般都很大，这就导致模型参数个数就会变得很大。为了解决这些问题他们提出了一个基于factorization的方法。他们没有直接把one-hot映射到hidden layer, 而是先把one-hot映射到低维空间之后，再映射到hidden layer。这其实类似于做了矩阵的分解。
  * **Cross-layer parameter sharing**. 每一层的layer可以共享参数，这样一来参数的个数不会以层数的增加而增加。所以最后得出来的模型相比BERT-large小18倍以上。
  *  **Inter-sentence coherence loss**. 在BERT的训练中提出了next sentence prediction loss, 也就是给定两个sentence segments, 然后让BERT去预测它俩之间的先后顺序，但在ALBERT文章里提出这种是有问题的，其实也说明这种训练方式用处不是很大。 所以他们做出了改进，他们使用的是setence-order prediction loss (SOP)，其实是基于主题的关联去预测是否两个句子调换了顺序。

#### 5.8  CNN和RNN的差别
* 训练速度上：CNN快很多。RNN慢的原因是每个timestep的计算，都要依赖前一个时刻的输出。而cnn的卷积的时候，和空间上其他的点没有任何联系，适合并行计算。
* 数据约束：CNN对于数据的约束就很强了，图像识别，input的纬度是48*48的，必须定死了，而RNN其实对于数据的长度（句子的长度）没有要求（TF里面有动态rnn来在输入rnn之前去掉pad为0的地方）
* 卷积层不同空间位置的神经元共享权值，用于发现图像中不同空间位置的模式。共享参数是深度学习一个重要的思想，其在减少网络参数的同时仍然能保持很高的网络容量(capacity)。卷积层在空间方向共享参数，而循环神经网络(recurrent neural networks)在时间方向共享参数。

#### 5.9  深度学习平台
* 阿里NASA计划的机器学习平台PAI（17年）
  * 全面兼容TF，Caffe，MXNet深度学习框架
  * 提供云端的计算资源
  * 集成很多机器学习算法（分类，回归，聚类）
  * 支持大规模的分布式数据训练
* 百度paddlepaddle飞桨(18年)
  * 支持大规模的分布式数据训练
  * 多平台部署
  * 产业级的开源模型库（语义理解，图像分类，目标检测，图像分割等多种场景）
* 微软Microsoft Custom Vision Services（17年）
  * 针对的是图像分类器
  * 提供迁移学习的模型
* 谷歌的Cloud AutoML
  * 针对的是图像分类器
  * 提供迁移学习的模型



